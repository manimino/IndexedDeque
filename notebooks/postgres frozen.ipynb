{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2477482",
   "metadata": {},
   "source": [
    "#### keep obj refs in buckets\n",
    "\n",
    "There's an optimization possible with the postgres stuff\n",
    "- if the bucket owns its objects, e.g. in list form\n",
    "- and we want the entire bucket (because it only has 1 key, and we want that key)\n",
    "- we could just grab all the objects and return them in one shot\n",
    "\n",
    "downside: if we have an arbitrary obj id, which bucket did it come from? we dunno\n",
    "need to drag obj refs along with the obj ids up to higher levels then\n",
    "\n",
    "so when I query a bucket, the return value looks like:\n",
    "\n",
    "```\n",
    "[\n",
    "  obj_ids: sorted np array\n",
    "  objs: list\n",
    "  all_true: bool, True if there's no mask needed because we still want every obj \n",
    "  mask: bool np array or None. \n",
    "]\n",
    "```\n",
    "\n",
    "Then we maintain a whole lot of those structures as we go up. Like we'll have a list of 'em that we wanna union() at the end of `match`, and another list for everything we wanna `exclude`. \n",
    "\n",
    "Problem: duplicated storage\n",
    " - We are now storing many obj references in each idx instead of just once overall. \n",
    " - RAM cost per index is higher by 8 bytes / index / item. \n",
    "\n",
    "#### Full match\n",
    "\n",
    "To handle a query like:\n",
    "\n",
    "`find(match={}, exclude={'something': 'small'})`\n",
    "\n",
    "we'll need to do a scan across one of the indices to get all items first. \n",
    "\n",
    "Optionally, we could preserve a dummy index that contains all items in one key, just for the \"I need everything\" query (e.g. when doing an `__iter__`.\n",
    "\n",
    "\n",
    "#### multiple matches / excludes\n",
    "\n",
    "Currently some queries are not expressible. Find me everything with `a=1 and [(b != 2) or (c != 3)]`. \n",
    "\n",
    "We could get one level deeper by accepting lists, like `match={a:1}, exclude=[{b: 2}, {c: 3}]`. Accepting list-of-list and so on could keep going deeper still. It makes a tree, which we can eval from the ground up.\n",
    "\n",
    "Not sure whether we wanna go down that road. Sounds long. \n",
    "\n",
    "Just something to keep in mind as we update the ol' query evaluation engine to fit this new approach.\n",
    "\n",
    "ON SECOND THOUGHT. We can index general functions. A user could just make an index on:\n",
    "\n",
    "```\n",
    "def not_these(obj):\n",
    "    return b != 2 or c != 3\n",
    "```\n",
    "\n",
    "And then\n",
    "```\n",
    "hi = HashIndex(items, on=['a', not_these]\n",
    "hi.find({'a': 1, not_these: True)\n",
    "```\n",
    "\n",
    "which would be more efficient anyway. So screw complexity. Yay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7b424ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from bisect import bisect_left\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from sortedcontainers import SortedDict\n",
    "from pympler.asizeof import asizeof\n",
    "import sortednp as snp\n",
    "from operator import itemgetter\n",
    "from typing import List, Optional, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb0d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474fd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa611043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MatchedArray:\n",
    "    \"\"\"\n",
    "    Return type of a FrozenFieldIndex. Contains objects and their ids in parallel arrays.\n",
    "    Implements efficient union / intersection operations for combining results.\n",
    "    \"\"\"\n",
    "    id_arr: np.ndarray # sorted np array of ints\n",
    "    obj_arr: np.ndarray # array of Python objects, sorted by obj_id\n",
    "    \n",
    "    def intersection(self, other:MatchedArray):\n",
    "        \n",
    "    \n",
    "    def union(self, other: MatchedArray):\n",
    "        \"\"\"This array becomes the union of self and other.\"\"\"\n",
    "        self.id_arr, indices = snp.merge(a0, a1, indices=True, duplicates=snp.DROP)\n",
    "        obj_arr = np.empty_like(merged_id_arr, dtype='O')\n",
    "        obj_arr[indices[0]] = o_arr_1\n",
    "        obj_arr[indices[1]] = o_arr_2\n",
    "        self.obj_arr = obj_arr\n",
    "    \n",
    "    def difference(self, other: MatchedArray):\n",
    "        \"\"\"Remove elements in other from self.\"\"\"\n",
    "        # TODO: use an array hit_counts instead\n",
    "        # each time an idx is in the intersection, += 1 it\n",
    "        # at the end, obj_arr = obj_arr[np.where(hit_counts == n_intersections)]\n",
    "        matched_positions = snp.intersect(self.id_arr, other.id_arr, indices=True)[1][0]\n",
    "        nonmatches = np.ones_like(self.delete_arr, dtype=bool)\n",
    "        nonmatches[matched_positions] = False\n",
    "        self.id_arr = self.id_arr[nonmatches]\n",
    "        self.obj_arr = self.obj_arr[nonmatches]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.id_arr)\n",
    "\n",
    "def intersect_all(marrs: List[MatchedArray]) -> MatchedArray:\n",
    "    \"\"\"Find the intersection of all MatchedArrays.\"\"\"\n",
    "    _, pos = min(len(m) for m in marrs)\n",
    "    for i, m in enumerate(marrs):\n",
    "        if i == pos:\n",
    "            continue\n",
    "        m[pos].difference(m[i])\n",
    "        if len(m[pos]) == 0:\n",
    "            break\n",
    "    return m[pos]\n",
    "\n",
    "def union_all(marrs: List[MatchedArray]):\n",
    "    \"\"\"Find the union of all MatchedArrays\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed32e0",
   "metadata": {},
   "source": [
    "### are obj arrays actually slow to make? is it worth doing the lazy eval here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "53f2f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10**6\n",
    "lots = np.array([str(i) for i in range(n)])\n",
    "\n",
    "n_masks = 10\n",
    "masks = []\n",
    "for k in range(n_masks):\n",
    "    mask = np.zeros_like(lots, dtype=bool)\n",
    "    for i in range(n):\n",
    "        if i % (k+1) == 0:\n",
    "            mask[i] = True\n",
    "    masks.append(mask)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "63c22454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022984027862548828\n"
     ]
    }
   ],
   "source": [
    "# just like copy them idk\n",
    "t0 = time.time()\n",
    "result = np.copy(lots)\n",
    "for m in masks:\n",
    "    result = np.copy(result[:int(0.9*len(result))])\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "31e5b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012212514877319336\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "result = np.copy(lots)\n",
    "for m in masks:\n",
    "    m1 = np.copy(mask)\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "00fdc012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89316e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e818e816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 5]), array([2, 3, 4, 4]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0 = np.array([5,6,7,99])\n",
    "a1 = np.array([7,8,9,9])\n",
    "o_arr_1 = np.array(['a', 'b', 'c','q'], dtype='O')\n",
    "o_arr_2 = np.array(['c', 'd', 'e','e'], dtype='O')\n",
    "\n",
    "merged_id_arr, indices = snp.merge(a0, a1, indices=True, duplicates=snp.DROP)\n",
    "obj_arr = np.empty_like(merged_id_arr, dtype='O')\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "159cb41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'b', 'c', 'd', 'e', 'q'], dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_arr[indices[0]] = o_arr_1\n",
    "obj_arr[indices[1]] = o_arr_2\n",
    "obj_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea46a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc10be05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'union' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_111334/1801076560.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'union' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328a76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc4452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4dd5f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_remove_nan_ids():\n",
    "    # mask bits typically get set after an intersect / difference operation\n",
    "    s = SomeObjs(obj_ids = np.array([1,2,3], dtype='uint64'), objs=['a', 'b', 'c'], all_true=True, mask=None)\n",
    "\n",
    "    s.all_true = False\n",
    "    # todo parameterize for 0, 1, 2, and 3 trues -- all different outcomes\n",
    "    s.mask = np.array([True, False, True], dtype=bool)\n",
    "\n",
    "    apply_mask(s)\n",
    "    assert s.objs == ['a', 'c']\n",
    "    \n",
    "test_apply_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecec9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_id(s: SomeObjs):\n",
    "    # mutates s\n",
    "    sort_order = s.obj_ids.argsort()\n",
    "    s.obj_ids = s.obj_ids[sort_order]\n",
    "    s.objs = s.objs[sort_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5bd2ca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3620735981.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_111334/3620735981.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    isect = snp.[1]\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def union_all(ls: List[SomeObjs]) -> SomeObjs:\n",
    "    # so we take the union'd IDs\n",
    "    # then we intersect them with each SomeObjs one at a time\n",
    "    # on intersect, we:\n",
    "    # - grab the obj and its id, add it to the output SomeObjs\n",
    "    # - nan out the copy of it in the unionset\n",
    "    # Last step, sort. We'll need a function for that.\n",
    "    result = SomeObjs()\n",
    "    union_ids = snp.kway_merge(*[s.obj_ids for s in ls], assume_sorted=True, duplicates=snp.DROP)\n",
    "    for s in ls:\n",
    "        isect_idxs = snp.[1]\n",
    "    \n",
    "    print(ids)\n",
    "\n",
    "s = SomeObjs(obj_ids = np.array([1,2,3], dtype='uint64'), objs=['a', 'b', 'c'], all_true=True, mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c85cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: use nans instead of a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intersect_all(ls: List[SomeObjs]):\n",
    "    pass\n",
    "\n",
    "def difference(objs: SomeObjs, not_these: SomeObjs):\n",
    "    pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
