{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60db33c1",
   "metadata": {},
   "source": [
    "```\n",
    "we want to change from\n",
    "\n",
    "lookup = {value: {objs}}\n",
    "\n",
    "to \n",
    "\n",
    "consistent_hash(value) = bucket_id\n",
    "lookup = {bucket_id: {objs}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e0ca5d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from sortedcontainers import SortedDict\n",
    "from pympler.asizeof import asizeof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "32cf1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_='''\n",
    "OK, so the implementation of choice is a \n",
    "SortedDict of {min_value: Bucket}\n",
    "and a heap of {size: min_value} containing the splittable buckets that are big.\n",
    "\n",
    "Bucket knows the keys it contains as well as their counts. \n",
    "If asked for a split point, it will give one that best bisects its keys. That's O(log(n)) probably.\n",
    "You can custom-write a bisection for that. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0625a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item:\n",
    "    def __init__(self):\n",
    "        self.s = random.choice(['mercury', 'venus', 'earth', 'mars', 'jupiter', 'saturn', 'uranus', 'neptune'])\n",
    "        self.x = random.random()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.s} {round(self.x, 2)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c86f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fieldidx():\n",
    "    def __init__(self):\n",
    "        self.n_buckets = 10\n",
    "        self.idx = dict(zip(range(self.n_buckets), \n",
    "                            [set() for _ in range(self.n_buckets)]))\n",
    "        self.size = 0\n",
    "        self.objs = dict()\n",
    "        self.field = 's'\n",
    "    \n",
    "    def _get_bucket_for(self, val):\n",
    "        h = hash(val)\n",
    "        bucket = h % len(self.idx)\n",
    "        return bucket\n",
    "    \n",
    "    def _resize(self):\n",
    "        pass\n",
    "    \n",
    "    def add(self, item):\n",
    "        ptr = id(item)\n",
    "        val = getattr(item, self.field, None)\n",
    "        bucket = self._get_bucket_for(val)\n",
    "        self.idx[bucket].add(ptr)\n",
    "        self.objs[ptr] = item\n",
    "        self.size += 1\n",
    "        \n",
    "    def get(self, val):\n",
    "        bucket = self._get_bucket_for(val)\n",
    "        matches = []\n",
    "        for obj_id in self.idx[bucket]:\n",
    "            obj = self.objs[obj_id]\n",
    "            obj_val = getattr(obj, self.field, None)\n",
    "            if obj_val is val or obj_val == val:\n",
    "                matches.append(obj)\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e24f632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "venus 0.16\n",
      "venus 0.93\n",
      "venus 0.75\n",
      "venus 0.21\n",
      "venus 0.69\n",
      "venus 0.04\n",
      "venus 0.05\n",
      "venus 0.56\n",
      "venus 0.72\n",
      "venus 0.42\n",
      "venus 0.86\n",
      "venus 0.14\n",
      "venus 0.05\n",
      "venus 0.21\n"
     ]
    }
   ],
   "source": [
    "idx = Fieldidx()\n",
    "for _ in range(100):\n",
    "    idx.add(Item())\n",
    "z = idx.get('venus')\n",
    "for i in z:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832b8f1",
   "metadata": {},
   "source": [
    "Best idea is:\n",
    "    \n",
    " - hash values to lie in some big range, like uint64\n",
    " - Initially, we have like 10 buckets containing even chunks of that range (pretend we have a good hash function...)\n",
    " - Maintain a data structure with easy max and min access (sorted deque? heaps? dict?) sorted by the number of elements stored in the bucket. Maybe have another one for n_keys or something too, we don't wanna keep trying to split one bucket that has a single high-card key in it. Ugh.\n",
    " - Anyway, split the biggest bucket when it comes time for adding more buckets. When do we add more buckets? Shit.\n",
    " - Wait. When a bucket is unsplittable, we could take it outta the list. It's its own thing now. That would work.\n",
    " - We might have to put it back in someday.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609761f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fce024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6d4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9e881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "800a9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize with 1 bucket spanning whole range\n",
    "# split when there are >1000 items in a splittable bucket\n",
    "n_bits_signed = sys.hash_info.hash_bits - 1  # typically 64 bits\n",
    "HASH_MIN = -2**n_bits_signed\n",
    "HASH_MAX = 2**n_bits_signed-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8882a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the mutable version of Bucket and ObjLookup. \n",
    "\n",
    "class Bucket:\n",
    "    def __init__(self):\n",
    "        self.obj_ids = set()  # uint64\n",
    "        self.val_hashes = set()  # int64 - which hashes are stored in this bucket\n",
    "    \n",
    "    def add(self, val_hash, obj_id):\n",
    "        self.val_hashes.add(val_hash)\n",
    "        self.obj_ids.add(obj_id)\n",
    "    \n",
    "    def update(self, new_val_hashes, new_obj_ids):\n",
    "        self.val_hashes = self.val_hashes.union(new_val_hashes)\n",
    "        self.obj_ids = self.obj_ids.union(new_obj_ids)\n",
    "    \n",
    "    def get_matching_objs(self, field, val, obj_lookup):\n",
    "        # look through all our obj_ids to see what matches this val, and return those.\n",
    "        objs = [obj_lookup.get(o) for o in self.obj_ids]\n",
    "        matches = []\n",
    "        # filter to just the ones that match. could use a filter() here maybe, or comprehension.\n",
    "        for obj in objs:\n",
    "            obj_val = getattr(obj, field, None)\n",
    "            if obj_val is val or obj_val == val:\n",
    "                matches.append(obj)\n",
    "        return matches\n",
    "    \n",
    "    def get_all_objs(self, obj_lookup):\n",
    "        return [obj_lookup.get(o) for o in self.obj_ids]\n",
    "    \n",
    "    def split(self, field, obj_lookup):\n",
    "        my_hashes = list(sorted(self.val_hashes))\n",
    "        # dump out the upper half of our hashes\n",
    "        half_point = len(my_hashes) // 2\n",
    "        dumped_hashes = set(my_hashes[half_point:])\n",
    "        \n",
    "        # dereference each object \n",
    "        # Find the objects with field_vals that hash to any of dumped_hashes\n",
    "        # we will move their ids to the new bucket\n",
    "        dumped_obj_ids = set()\n",
    "        for obj_id in list(self.obj_ids):\n",
    "            obj = obj_lookup.get(obj_id)\n",
    "            obj_val = getattr(obj, field, None)\n",
    "            if hash(obj_val) in dumped_hashes:\n",
    "                dumped_obj_ids.add(obj_id)\n",
    "                self.obj_ids.remove(obj_id)\n",
    "        for dh in dumped_hashes:\n",
    "            self.val_hashes.remove(dh)\n",
    "        return dumped_hashes, dumped_obj_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.obj_ids)\n",
    "    \n",
    "\n",
    "class ObjLookup:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.objs = dict()\n",
    "        \n",
    "    def get(self, obj_id):\n",
    "        return self.objs.get(obj_id)\n",
    "    \n",
    "    def set(self, obj_id, obj):\n",
    "        self.objs[obj_id] = obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "8bd698a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_THRESH_UPPER = 30\n",
    "SIZE_THRESH_LOWER = 3\n",
    "\n",
    "class Field:\n",
    "    # Stores the possible values of this field in a set of buckets\n",
    "    # Several values may be allocated to the same bucket for space efficiency reasons\n",
    "    def __init__(self, field):\n",
    "        self.buckets = SortedDict()  # O(1) add / remove, O(log(n)) find bucket for key\n",
    "        self.buckets[HASH_MIN] =  Bucket()  # always contains at least one bucket\n",
    "        self.objs = ObjLookup()\n",
    "        self.field = field\n",
    "    \n",
    "    def get(self, field_value):\n",
    "        val_hash = hash(field_value)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        return self.buckets[k].get_matching_objs(self.field, field_value, self.objs)\n",
    "        \n",
    "    def _get_bucket_key_for(self, val_hash):\n",
    "        list_idx = self.buckets.bisect_right(val_hash) - 1\n",
    "        k, _ = self.buckets.peekitem(list_idx)\n",
    "        return k\n",
    "        \n",
    "    def add(self, obj):\n",
    "        field_value = getattr(obj, self.field, None)\n",
    "        val_hash = hash(field_value)\n",
    "        obj_id = id(obj)\n",
    "        self.objs.set(obj_id, obj)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        self.buckets[k].add(val_hash, obj_id)\n",
    "        # split bucket if it's big and contains more than one key\n",
    "        if len(self.buckets[k]) > SIZE_THRESH_UPPER and len(self.buckets[k].val_hashes) >= 2:\n",
    "            new_hashes, new_obj_ids = self.buckets[k].split(self.field, self.objs)\n",
    "            new_bucket = Bucket()\n",
    "            new_bucket.update(new_hashes, new_obj_ids)\n",
    "            self.buckets[min(new_hashes)] = new_bucket\n",
    "    \n",
    "    def remove(self, field_value, obj_id):\n",
    "        k = self._get_bucket_key_for(field_value)\n",
    "        self.buckets[k].remove(key, obj_id)\n",
    "        if self.buckets[k].size < SIZE_THRESH_LOWER and k != HASH_MIN:\n",
    "            # try and merge it with its neighbor to the left\n",
    "            idx_j = self.buckets.bisect_left(k-1)\n",
    "            j, _ = self.buckets.peekitem(idx_j)\n",
    "            if self.buckets[j].size + self.buckets[i].size < SIZE_THRESH_UPPER:\n",
    "                self.buckets[j].update(self.buckets[i].stuff)\n",
    "                del self.buckets[i]\n",
    "                \n",
    "    def bucket_report(self):\n",
    "        ls = []\n",
    "        for bkey in self.buckets:\n",
    "            bucket = self.buckets[bkey]\n",
    "            bset = set()\n",
    "            for o in bucket.get_all_objs(self.objs):\n",
    "                bset.add(getattr(o, self.field))\n",
    "            ls.append((bkey, bset))\n",
    "        return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "e91d4ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mars 0.43\n",
      "mars 0.45\n",
      "mars 0.34\n",
      "mars 0.98\n",
      "mars 0.65\n",
      "mars 0.09\n",
      "mars 0.7\n",
      "mars 0.05\n",
      "mars 0.44\n",
      "mars 0.91\n",
      "mars 0.64\n",
      "[-9223372036854775808, -1783096510027627915, -588565948556953553, 924058623898380127]\n",
      "(-9223372036854775808, {'earth', 'venus'})\n",
      "(-1783096510027627915, {'mars', 'uranus'})\n",
      "(-588565948556953553, {'saturn', 'mercury'})\n",
      "(924058623898380127, {'neptune', 'jupiter'})\n"
     ]
    }
   ],
   "source": [
    "idx = Field('s')\n",
    "for _ in range(100):\n",
    "    idx.add(Item())\n",
    "z = idx.get('mars')\n",
    "ct = 0\n",
    "for i in z:\n",
    "    print(i)\n",
    "    ct += 1\n",
    "    if ct > 10:\n",
    "        break\n",
    "print(sorted(idx.buckets.keys()))\n",
    "for b in idx.bucket_report():\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a7a23778",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_74121/76139197.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "asizeof(Bucket(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ad59de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3d47a383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356 µs ± 241 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "np.zeros((10**6,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9cb9bbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 ms ± 1.69 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "ls = [0 for _ in range(10**6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "11587b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [0 for _ in range(10**6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0ccfb817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727 µs ± 317 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "del ls[10**5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "aef62bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't really use lists at the rate we want to add / remove nodes. At 1ms per add / remove, that's 1s just to \n",
    "# build a 1M-node bucket list for a single key. Too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d9a32c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d6139b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.64 ms ± 491 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "s = SortedDict()\n",
    "for i in range(1000, -1, -1):\n",
    "    s[i] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "81797a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.850229024887085\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "s = SortedDict()\n",
    "for i in range(10**6, -1, -1):\n",
    "    s[i] = Bucket(i)\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f208cea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it costs 458 bytes per bucket to maintain this structure.\n",
    "# That works out to 0.5 bytes per item, figuring 1000 items in each bucket. \n",
    "# Not disastrous.\n",
    "round(asizeof(s) / 10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "36c5f986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.97 µs ± 3.1 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "s.bisect_left(random.choice(range(10**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7cdedb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.45 µs ± 1.08 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "bisect_left(ls, random.choice(range(10**5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "107bbd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.33 µs ± 1.93 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "del s[random.choice(range(10**5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf7b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
