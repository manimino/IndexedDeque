{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1a7d06",
   "metadata": {},
   "source": [
    "```\n",
    "we want to change from\n",
    "\n",
    "lookup = {value: {objs}}\n",
    "\n",
    "to \n",
    "\n",
    "consistent_hash(value) = bucket_id\n",
    "lookup = {bucket_id: {objs}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "1ef5c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from sortedcontainers import SortedDict\n",
    "from pympler.asizeof import asizeof\n",
    "import sortednp as snp\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "eec2470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_='''\n",
    "OK, so the implementation of choice is a \n",
    "SortedDict of {min_value: Bucket}\n",
    "and a heap of {size: min_value} containing the splittable buckets that are big.\n",
    "\n",
    "Bucket knows the keys it contains as well as their counts. \n",
    "If asked for a split point, it will give one that best bisects its keys. That's O(log(n)) probably.\n",
    "You can custom-write a bisection for that. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "5825afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANETS = ['mercury']*10000 + ['venus']*100 + ['earth', 'mars', 'jupiter', 'saturn', 'uranus', 'neptune']\n",
    "class Item:\n",
    "    def __init__(self):\n",
    "        self.s = random.choice(PLANETS)\n",
    "        self.x = random.random()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.s} {round(self.x, 2)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e9bfafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fieldidx():\n",
    "    def __init__(self):\n",
    "        self.n_buckets = 10\n",
    "        self.idx = dict(zip(range(self.n_buckets), \n",
    "                            [set() for _ in range(self.n_buckets)]))\n",
    "        self.size = 0\n",
    "        self.objs = dict()\n",
    "        self.field = 's'\n",
    "    \n",
    "    def _get_bucket_for(self, val):\n",
    "        h = hash(val)\n",
    "        bucket = h % len(self.idx)\n",
    "        return bucket\n",
    "    \n",
    "    def _resize(self):\n",
    "        pass\n",
    "    \n",
    "    def add(self, item):\n",
    "        ptr = id(item)\n",
    "        val = getattr(item, self.field, None)\n",
    "        bucket = self._get_bucket_for(val)\n",
    "        self.idx[bucket].add(ptr)\n",
    "        self.objs[ptr] = item\n",
    "        self.size += 1\n",
    "        \n",
    "    def get(self, val):\n",
    "        bucket = self._get_bucket_for(val)\n",
    "        matches = []\n",
    "        for obj_id in self.idx[bucket]:\n",
    "            obj = self.objs[obj_id]\n",
    "            obj_val = getattr(obj, self.field, None)\n",
    "            if obj_val is val or obj_val == val:\n",
    "                matches.append(obj)\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "762a5410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "venus 0.16\n",
      "venus 0.93\n",
      "venus 0.75\n",
      "venus 0.21\n",
      "venus 0.69\n",
      "venus 0.04\n",
      "venus 0.05\n",
      "venus 0.56\n",
      "venus 0.72\n",
      "venus 0.42\n",
      "venus 0.86\n",
      "venus 0.14\n",
      "venus 0.05\n",
      "venus 0.21\n"
     ]
    }
   ],
   "source": [
    "idx = Fieldidx()\n",
    "for _ in range(100):\n",
    "    idx.add(Item())\n",
    "z = idx.get('venus')\n",
    "for i in z:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd6c4f",
   "metadata": {},
   "source": [
    "Best idea is:\n",
    "    \n",
    " - hash values to lie in some big range, like uint64\n",
    " - Initially, we have like 10 buckets containing even chunks of that range (pretend we have a good hash function...)\n",
    " - Maintain a data structure with easy max and min access (sorted deque? heaps? dict?) sorted by the number of elements stored in the bucket. Maybe have another one for n_keys or something too, we don't wanna keep trying to split one bucket that has a single high-card key in it. Ugh.\n",
    " - Anyway, split the biggest bucket when it comes time for adding more buckets. When do we add more buckets? Shit.\n",
    " - Wait. When a bucket is unsplittable, we could take it outta the list. It's its own thing now. That would work.\n",
    " - We might have to put it back in someday.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c9500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c2c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7215ffad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0a766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a0a6da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize with 1 bucket spanning whole range\n",
    "# split when there are >1000 items in a splittable bucket\n",
    "n_bits_signed = sys.hash_info.hash_bits - 1  # typically 64 bits\n",
    "HASH_MIN = -2**n_bits_signed\n",
    "HASH_MAX = 2**n_bits_signed-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b920a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the mutable version of Bucket and ObjLookup. \n",
    "\n",
    "class Bucket:\n",
    "    def __init__(self):\n",
    "        self.obj_ids = set()  # uint64\n",
    "        self.val_hashes = set()  # int64 - which hashes are stored in this bucket\n",
    "    \n",
    "    def add(self, val_hash, obj_id):\n",
    "        self.val_hashes.add(val_hash)\n",
    "        self.obj_ids.add(obj_id)\n",
    "    \n",
    "    def update(self, new_val_hashes, new_obj_ids):\n",
    "        self.val_hashes = self.val_hashes.union(new_val_hashes)\n",
    "        self.obj_ids = self.obj_ids.union(new_obj_ids)\n",
    "    \n",
    "    def get_matching_objs(self, field, val, obj_lookup):\n",
    "        # look through all our obj_ids to see what matches this val, and return those.\n",
    "        objs = [obj_lookup.get(o) for o in self.obj_ids]\n",
    "        matches = []\n",
    "        # filter to just the ones that match. could use a filter() here maybe, or comprehension.\n",
    "        for obj in objs:\n",
    "            obj_val = getattr(obj, field, None)\n",
    "            if obj_val is val or obj_val == val:\n",
    "                matches.append(obj)\n",
    "        return matches\n",
    "    \n",
    "    def get_all_objs(self, obj_lookup):\n",
    "        return [obj_lookup.get(o) for o in self.obj_ids]\n",
    "    \n",
    "    def split(self, field, obj_lookup):\n",
    "        my_hashes = list(sorted(self.val_hashes))\n",
    "        # dump out the upper half of our hashes\n",
    "        half_point = len(my_hashes) // 2\n",
    "        dumped_hashes = set(my_hashes[half_point:])\n",
    "        \n",
    "        # dereference each object \n",
    "        # Find the objects with field_vals that hash to any of dumped_hashes\n",
    "        # we will move their ids to the new bucket\n",
    "        dumped_obj_ids = set()\n",
    "        for obj_id in list(self.obj_ids):\n",
    "            obj = obj_lookup.get(obj_id)\n",
    "            obj_val = getattr(obj, field, None)\n",
    "            if hash(obj_val) in dumped_hashes:\n",
    "                dumped_obj_ids.add(obj_id)\n",
    "                self.obj_ids.remove(obj_id)\n",
    "        for dh in dumped_hashes:\n",
    "            self.val_hashes.remove(dh)\n",
    "        return dumped_hashes, dumped_obj_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.obj_ids)\n",
    "    \n",
    "\n",
    "class ObjLookup:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.objs = dict()\n",
    "        \n",
    "    def get(self, obj_id):\n",
    "        return self.objs.get(obj_id)\n",
    "    \n",
    "    def set(self, obj_id, obj):\n",
    "        self.objs[obj_id] = obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "c646a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_THRESH_UPPER = 17\n",
    "SIZE_THRESH_LOWER = 3\n",
    "\n",
    "class Field:\n",
    "    # Stores the possible values of this field in a set of buckets\n",
    "    # Several values may be allocated to the same bucket for space efficiency reasons\n",
    "    def __init__(self, field):\n",
    "        self.buckets = SortedDict()  # O(1) add / remove, O(log(n)) find bucket for key\n",
    "        self.buckets[HASH_MIN] =  Bucket()  # always contains at least one bucket\n",
    "        self.objs = ObjLookup()\n",
    "        self.field = field\n",
    "    \n",
    "    def get(self, field_value):\n",
    "        val_hash = hash(field_value)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        return self.buckets[k].get_matching_objs(self.field, field_value, self.objs)\n",
    "        \n",
    "    def _get_bucket_key_for(self, val_hash):\n",
    "        list_idx = self.buckets.bisect_right(val_hash) - 1\n",
    "        k, _ = self.buckets.peekitem(list_idx)\n",
    "        return k\n",
    "        \n",
    "    def add(self, obj):\n",
    "        field_value = getattr(obj, self.field, None)\n",
    "        val_hash = hash(field_value)\n",
    "        obj_id = id(obj)\n",
    "        self.objs.set(obj_id, obj)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        self.buckets[k].add(val_hash, obj_id)\n",
    "        # split bucket if it's big and contains more than one key\n",
    "        if len(self.buckets[k]) > SIZE_THRESH_UPPER and len(self.buckets[k].val_hashes) >= 2:\n",
    "            new_hashes, new_obj_ids = self.buckets[k].split(self.field, self.objs)\n",
    "            new_bucket = Bucket()\n",
    "            new_bucket.update(new_hashes, new_obj_ids)\n",
    "            self.buckets[min(new_hashes)] = new_bucket\n",
    "    \n",
    "    def remove(self, field_value, obj_id):\n",
    "        k = self._get_bucket_key_for(field_value)\n",
    "        self.buckets[k].remove(key, obj_id)\n",
    "        if self.buckets[k].size < SIZE_THRESH_LOWER and k != HASH_MIN:\n",
    "            # try and merge it with its neighbor to the left\n",
    "            idx_j = self.buckets.bisect_left(k-1)\n",
    "            j, _ = self.buckets.peekitem(idx_j)\n",
    "            if self.buckets[j].size + self.buckets[i].size < SIZE_THRESH_UPPER:\n",
    "                self.buckets[j].update(self.buckets[i].stuff)\n",
    "                del self.buckets[i]\n",
    "                \n",
    "    def bucket_report(self):\n",
    "        ls = []\n",
    "        for bkey in self.buckets:\n",
    "            bucket = self.buckets[bkey]\n",
    "            bset = set()\n",
    "            for o in bucket.get_all_objs(self.objs):\n",
    "                bset.add(getattr(o, self.field))\n",
    "            ls.append((bkey, bset, len(bucket)))\n",
    "        return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "eac510eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = Field('s')\n",
    "n = 10**6\n",
    "items = [Item() for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "98c8e44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding 1000000 items\n",
      "\n",
      " 2.618 seconds to build this field thing\n",
      "\n",
      "[-9223372036854775808, -4657188873304669324, -1783096510027627915, -1390440632094239304, -588565948556953553, 288469824212975131, 924058623898380127, 4070670315453980499]\n",
      "(-9223372036854775808, {'earth'}, 88)\n",
      "(-4657188873304669324, {'venus'}, 9742)\n",
      "(-1783096510027627915, {'uranus'}, 87)\n",
      "(-1390440632094239304, {'mars'}, 85)\n",
      "(-588565948556953553, {'mercury'}, 989715)\n",
      "(288469824212975131, {'saturn'}, 94)\n",
      "(924058623898380127, {'jupiter'}, 94)\n",
      "(4070670315453980499, {'neptune'}, 95)\n"
     ]
    }
   ],
   "source": [
    "print('adding', n, 'items')\n",
    "t0 = time.time()\n",
    "for item in items:\n",
    "    idx.add(item)\n",
    "t1 = time.time()\n",
    "print('\\n', round(t1-t0,3), 'seconds to build this field thing\\n')\n",
    "print(sorted(idx.buckets.keys()))\n",
    "for b in idx.bucket_report():\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "642a7a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8671374320983887 seconds to build a HashIndex\n"
     ]
    }
   ],
   "source": [
    "from hashindex import HashIndex\n",
    "\n",
    "t0 = time.time()\n",
    "hi = HashIndex(items, on='s')\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a HashIndex')\n",
    "\n",
    "hi.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "7caeeef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1698291301727295 seconds to build a dict\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "d = dict()\n",
    "for i in items:\n",
    "    if i.s not in d:\n",
    "        d[i.s] = list()\n",
    "    d[i.s].append(i)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "50839f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'mercury'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "4e7ff402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ms ± 1.34 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = hi.find(match={'s': planet})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "f81d1598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 ms ± 7.66 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = idx.get(planet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "33ae5cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 ns ± 99.4 ns per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = d.get(planet)\n",
    "# yikes - how is this 1000x faster? something has gone really wrong here! let's see if it's the deref lookup that's\n",
    "# costing so much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "cb16a43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41048765182495117 seconds to build a deref dict\n"
     ]
    }
   ],
   "source": [
    "class DerefDict():\n",
    "    \n",
    "    def __init__(self, items):\n",
    "        self.objs = {id(item): item for item in items}\n",
    "        self.d = dict()\n",
    "        for i in items:\n",
    "            if i.s not in self.d:\n",
    "                self.d[i.s] = list()\n",
    "            self.d[i.s].append(id(i))\n",
    "    \n",
    "    def get(self, val):\n",
    "        ids = self.d.get(val)\n",
    "        return [self.objs.get(i) for i in ids]\n",
    "\n",
    "t0 = time.time()\n",
    "dd  = DerefDict(items)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a deref dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0d0d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "6e30195b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 ms ± 13.4 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = dd.get(planet)\n",
    "# the difference is that you are doing len(planets) dict lookups instead of just one dict lookup.\n",
    "# Can we keep the list literal around during processing instead?\n",
    "# e.g. - most of the time we will want an entire list (simple lookup, no intersection). Detect that \n",
    "# case and we've got something as good as dict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "5fc643e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4190680980682373 seconds to build a deref dict, listy edition\n"
     ]
    }
   ],
   "source": [
    "class DerefDictListy():\n",
    "    def __init__(self, items):\n",
    "        self.objs = []\n",
    "        self.d = dict()\n",
    "        for i in items:\n",
    "            if i.s not in self.d:\n",
    "                self.d[i.s] = list()\n",
    "            self.d[i.s].append(len(self.objs))\n",
    "            self.objs.append(obj)\n",
    "    \n",
    "    def get(self, val):\n",
    "        ids = self.d.get(val)\n",
    "        return itemgetter(*ids)(self.objs)\n",
    "\n",
    "t0 = time.time()\n",
    "ddl  = DerefDict(items)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a deref dict, listy edition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "e96de993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.2 µs ± 4.89 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = ddl.get(planet)\n",
    "# the difference is that you are doing len(planets) dict lookups instead of just one dict lookup.\n",
    "# Can we keep the list literal around during processing instead?\n",
    "# e.g. - most of the time we will want an entire list (simple lookup, no intersection). Detect that \n",
    "# case and we've got something as good as dict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "b0401fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('b', 'c')"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list('abcdefg')\n",
    "isect_ids = snp.intersect(np.array([2,3,6]), np.array([1,2,3,4,5]), indices=True)[1][1]\n",
    "itemgetter(*isect_ids)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "656b7b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<__main__.Item object at 0x7f1cb7a44af0>,\n",
       "       <__main__.Item object at 0x7f1cb7aca1f0>,\n",
       "       <__main__.Item object at 0x7f1cb7ede310>,\n",
       "       <__main__.Item object at 0x7f1cb83a4790>,\n",
       "       <__main__.Item object at 0x7f1cb8530af0>,\n",
       "       <__main__.Item object at 0x7f1cb88c6370>,\n",
       "       <__main__.Item object at 0x7f1cb88c8df0>,\n",
       "       <__main__.Item object at 0x7f1cb91a0b50>,\n",
       "       <__main__.Item object at 0x7f1cb92c80d0>,\n",
       "       <__main__.Item object at 0x7f1cb93a2cd0>,\n",
       "       <__main__.Item object at 0x7f1cb94045b0>,\n",
       "       <__main__.Item object at 0x7f1cb9488790>,\n",
       "       <__main__.Item object at 0x7f1cb9656190>,\n",
       "       <__main__.Item object at 0x7f1cb9664190>,\n",
       "       <__main__.Item object at 0x7f1cb99001f0>,\n",
       "       <__main__.Item object at 0x7f1cb9952970>,\n",
       "       <__main__.Item object at 0x7f1cb9b96bb0>,\n",
       "       <__main__.Item object at 0x7f1cb9bc6790>,\n",
       "       <__main__.Item object at 0x7f1cb9c4a5b0>,\n",
       "       <__main__.Item object at 0x7f1cba080610>,\n",
       "       <__main__.Item object at 0x7f1cba1222b0>,\n",
       "       <__main__.Item object at 0x7f1cba478fd0>,\n",
       "       <__main__.Item object at 0x7f1cba72a9d0>,\n",
       "       <__main__.Item object at 0x7f1cba82aa90>,\n",
       "       <__main__.Item object at 0x7f1cba834310>,\n",
       "       <__main__.Item object at 0x7f1cba8dc9d0>,\n",
       "       <__main__.Item object at 0x7f1cbaa04550>,\n",
       "       <__main__.Item object at 0x7f1cbab24490>,\n",
       "       <__main__.Item object at 0x7f1cbabf60d0>,\n",
       "       <__main__.Item object at 0x7f1cbb19c850>,\n",
       "       <__main__.Item object at 0x7f1cbb520550>,\n",
       "       <__main__.Item object at 0x7f1cbb5b4430>,\n",
       "       <__main__.Item object at 0x7f1cbb7144f0>,\n",
       "       <__main__.Item object at 0x7f1cbb8ded90>,\n",
       "       <__main__.Item object at 0x7f1cbc0c4e50>,\n",
       "       <__main__.Item object at 0x7f1cbc38c670>,\n",
       "       <__main__.Item object at 0x7f1cbc624250>,\n",
       "       <__main__.Item object at 0x7f1cbca80550>,\n",
       "       <__main__.Item object at 0x7f1cbcb7a3d0>,\n",
       "       <__main__.Item object at 0x7f1cbcfeae50>,\n",
       "       <__main__.Item object at 0x7f1cbd144730>,\n",
       "       <__main__.Item object at 0x7f1cbd3441f0>,\n",
       "       <__main__.Item object at 0x7f1cbd3ac310>,\n",
       "       <__main__.Item object at 0x7f1cbd5c26d0>,\n",
       "       <__main__.Item object at 0x7f1cbd5f4850>,\n",
       "       <__main__.Item object at 0x7f1cbd7228b0>,\n",
       "       <__main__.Item object at 0x7f1cbda187f0>,\n",
       "       <__main__.Item object at 0x7f1cbdbf8790>,\n",
       "       <__main__.Item object at 0x7f1cbdc72eb0>,\n",
       "       <__main__.Item object at 0x7f1cbe1fa250>,\n",
       "       <__main__.Item object at 0x7f1cbe24c2b0>,\n",
       "       <__main__.Item object at 0x7f1cbe686970>,\n",
       "       <__main__.Item object at 0x7f1cbe7d4eb0>,\n",
       "       <__main__.Item object at 0x7f1cbe8dc190>,\n",
       "       <__main__.Item object at 0x7f1cbe918790>,\n",
       "       <__main__.Item object at 0x7f1cbecc6b50>,\n",
       "       <__main__.Item object at 0x7f1cbefb6a90>,\n",
       "       <__main__.Item object at 0x7f1cbf54a7f0>,\n",
       "       <__main__.Item object at 0x7f1cbfadef70>,\n",
       "       <__main__.Item object at 0x7f1cbfcd0850>,\n",
       "       <__main__.Item object at 0x7f1cbfef16d0>,\n",
       "       <__main__.Item object at 0x7f1cbff23310>,\n",
       "       <__main__.Item object at 0x7f1cc00ef7f0>,\n",
       "       <__main__.Item object at 0x7f1cc0287fd0>,\n",
       "       <__main__.Item object at 0x7f1cc0743fd0>,\n",
       "       <__main__.Item object at 0x7f1cc074f970>,\n",
       "       <__main__.Item object at 0x7f1cc088a370>,\n",
       "       <__main__.Item object at 0x7f1cc0901a30>,\n",
       "       <__main__.Item object at 0x7f1cc0d01190>,\n",
       "       <__main__.Item object at 0x7f1cc1065130>,\n",
       "       <__main__.Item object at 0x7f1cc13ac130>,\n",
       "       <__main__.Item object at 0x7f1cc14a7190>,\n",
       "       <__main__.Item object at 0x7f1cc162deb0>,\n",
       "       <__main__.Item object at 0x7f1cc1680430>,\n",
       "       <__main__.Item object at 0x7f1cc18092b0>,\n",
       "       <__main__.Item object at 0x7f1cc1acb670>,\n",
       "       <__main__.Item object at 0x7f1cc1c457f0>,\n",
       "       <__main__.Item object at 0x7f1cc1f3d130>,\n",
       "       <__main__.Item object at 0x7f1cc2087070>,\n",
       "       <__main__.Item object at 0x7f1cc21af7f0>,\n",
       "       <__main__.Item object at 0x7f1cc22b28b0>,\n",
       "       <__main__.Item object at 0x7f1cc23adcd0>,\n",
       "       <__main__.Item object at 0x7f1cc2777df0>,\n",
       "       <__main__.Item object at 0x7f1cc27baf10>,\n",
       "       <__main__.Item object at 0x7f1cc2877550>,\n",
       "       <__main__.Item object at 0x7f1cc29f1f10>,\n",
       "       <__main__.Item object at 0x7f1cc31fa490>,\n",
       "       <__main__.Item object at 0x7f1cc3517370>], dtype=object)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = hi.find({'s': planet})\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd561cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
