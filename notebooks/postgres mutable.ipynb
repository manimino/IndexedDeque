{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "febda526",
   "metadata": {},
   "source": [
    "```\n",
    "we want to change from\n",
    "\n",
    "lookup = {value: {objs}}\n",
    "\n",
    "to \n",
    "\n",
    "consistent_hash(value) = bucket_id\n",
    "lookup = {bucket_id: {objs}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c6aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from sortedcontainers import SortedDict\n",
    "from pympler.asizeof import asizeof\n",
    "import sortednp as snp\n",
    "from cykhash import Int64Set\n",
    "from operator import itemgetter\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a184088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_='''\n",
    "OK, so the implementation of choice is a \n",
    "SortedDict of {min_value: Bucket}\n",
    "and a heap of {size: min_value} containing the splittable buckets that are big.\n",
    "\n",
    "Bucket knows the keys it contains as well as their counts. \n",
    "If asked for a split point, it will give one that best bisects its keys. That's O(log(n)) probably.\n",
    "You can custom-write a bisection for that. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f37d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANETS = ['mercury']*10000 + ['venus']*100 + ['earth', 'mars', 'jupiter', 'saturn', 'uranus', 'neptune']\n",
    "class Item:\n",
    "    def __init__(self):\n",
    "        self.s = random.choice(PLANETS)\n",
    "        self.x = random.random()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.s} {round(self.x, 2)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df987562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4b69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65a93535",
   "metadata": {},
   "source": [
    "Best idea is:\n",
    "    \n",
    " - hash values to lie in some big range, like uint64\n",
    " - Initially, we have like 10 buckets containing even chunks of that range (pretend we have a good hash function...)\n",
    " - Maintain a data structure with easy max and min access (sorted deque? heaps? dict?) sorted by the number of elements stored in the bucket. Maybe have another one for n_keys or something too, we don't wanna keep trying to split one bucket that has a single high-card key in it. Ugh.\n",
    " - Anyway, split the biggest bucket when it comes time for adding more buckets. When do we add more buckets? Shit.\n",
    " - Wait. When a bucket is unsplittable, we could take it outta the list. It's its own thing now. That would work.\n",
    " - We might have to put it back in someday.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88216bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bedb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c84948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cab743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05f06f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize with 1 bucket spanning whole range\n",
    "# split when there are >1000 items in a splittable bucket\n",
    "n_bits_signed = sys.hash_info.hash_bits - 1  # typically 64 bits\n",
    "HASH_MIN = -2**n_bits_signed\n",
    "HASH_MAX = 2**n_bits_signed-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ef5657c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_THRESH = 300\n",
    "\n",
    "class HashBucket:\n",
    "    \"\"\"\n",
    "    A HashBucket contains all obj_ids that have value hashes between some min and max value.\n",
    "    When the number of items in a HashBucket reaches SIZE_THRESH, the bucket will be split\n",
    "    into two buckets.\n",
    "    If a bucket ever gets empty, delete it unless it's the leftmost one -- we need at least one\n",
    "    always.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.obj_ids = set()  # uint64\n",
    "        self.val_hash_counts = dict()  # {int64: int64} - which hashes are stored in this bucket\n",
    "    \n",
    "    def add(self, val_hash, obj_id):\n",
    "        count = self.val_hash_counts.get(val_hash, 0)\n",
    "        self.val_hash_counts[val_hash] = count+1\n",
    "        self.obj_ids.add(obj_id)\n",
    "            \n",
    "    def update(self, new_val_hash_counts, new_obj_ids):\n",
    "        for v, c in new_val_hash_counts.items():\n",
    "            count = self.val_hash_counts.get(v, 0)\n",
    "            self.val_hash_counts[v] = count + c\n",
    "        self.obj_ids = self.obj_ids.union(new_obj_ids)\n",
    "\n",
    "    def get_all_ids(self):\n",
    "        return self.obj_ids\n",
    "    \n",
    "    def remove(self, val_hash, obj_id):\n",
    "        # todo handle exceptions\n",
    "        self.val_hash_counts[val_hash] -= 1\n",
    "        self.obj_ids.remove(obj_id)\n",
    "    \n",
    "    def split(self, field, obj_lookup):\n",
    "        my_hashes = list(sorted(self.val_hash_counts.keys()))\n",
    "        # dump out the upper half of our hashes\n",
    "        half_point = len(my_hashes) // 2\n",
    "        dumped_hash_counts = {v: self.val_hash_counts[v] for v in my_hashes[half_point:]}\n",
    "        \n",
    "        # dereference each object \n",
    "        # Find the objects with field_vals that hash to any of dumped_hashes\n",
    "        # we will move their ids to the new bucket\n",
    "        dumped_obj_ids = set()\n",
    "        for obj_id in list(self.obj_ids):\n",
    "            obj = obj_lookup.get(obj_id)\n",
    "            obj_val = getattr(obj, field, None)\n",
    "            if hash(obj_val) in dumped_hash_counts:\n",
    "                dumped_obj_ids.add(obj_id)\n",
    "                self.obj_ids.remove(obj_id)\n",
    "        for dh in dumped_hash_counts:\n",
    "            del self.val_hash_counts[dh]\n",
    "        return dumped_hash_counts, dumped_obj_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.obj_ids)\n",
    "    \n",
    "\n",
    "class DictBucket:\n",
    "    \"\"\"\n",
    "    A DictBucket stores object ids corresponding to only one val_hash. Note that multiple values\n",
    "    coult have the same val_hash (collision).\n",
    "    It stores all entries in a dict of {val: obj_id_set}, so it supports lookup by field value.\n",
    "    This makes finding objects by val very fast. Unlike a HashBucket, we don't have to dereference\n",
    "    all the objects and check their values during a find(). \n",
    "    DictBucket is great when many objects have the same val. \n",
    "    \"\"\"\n",
    "    def __init__(self, val_hash, obj_ids, obj_lookup, field):\n",
    "        self.val_hash = val_hash\n",
    "        self.d = dict()\n",
    "        for obj_id in obj_ids:\n",
    "            obj = obj_lookup.get(obj_id)\n",
    "            val = getattr(obj, field, None)\n",
    "            if val in self.d:\n",
    "                self.d[val].add(obj_id)\n",
    "            else:\n",
    "                self.d[val] = set([obj_id])\n",
    "    \n",
    "    def add(self, val, obj_id):\n",
    "        obj_ids = self.d.get(val, Int64Set())\n",
    "        obj_ids.add(obj_id)\n",
    "        \n",
    "    def remove(self, val, obj_id):\n",
    "        if val not in self.d:\n",
    "            raise KeyError('Object value not in here')\n",
    "        if obj_id not in self.d[val]:\n",
    "            raise KeyError('Object ID not in here')\n",
    "        self.d[val].remove(obj_id)\n",
    "        if len(self.d[val]) == 0:\n",
    "            del self.d[val]\n",
    "\n",
    "    def get_matching_ids(self, val):\n",
    "        return self.d[val]\n",
    "    \n",
    "    def get_all_ids(self):\n",
    "        return set.union(*self.d.values())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(len(s) for s in self.d.values())\n",
    "    \n",
    "\n",
    "class ObjLookup:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.objs = dict()\n",
    "        \n",
    "    def get(self, obj_id):\n",
    "        return self.objs.get(obj_id)\n",
    "    \n",
    "    def add(self, obj_id, obj):\n",
    "        self.objs[obj_id] = obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1b36b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutableFieldIndex:\n",
    "    # Stores the possible values of this field in a set of buckets\n",
    "    # Several values may be allocated to the same bucket for space efficiency reasons\n",
    "\n",
    "    def __init__(self, field: Union[Callable, str]):\n",
    "        self.buckets = SortedDict()  # O(1) add / remove, O(log(n)) find bucket for key\n",
    "        self.buckets[HASH_MIN] = HashBucket()  # always contains at least one bucket\n",
    "        self.objs = ObjLookup()\n",
    "        self.field = field\n",
    "    \n",
    "    def get_objs(self, val):\n",
    "        val_hash = hash(val)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        bucket = self.buckets[k]\n",
    "        \n",
    "        if isinstance(bucket, DictBucket):\n",
    "            return [self.objs.get(obj_id) for obj_id in bucket.get_matching_ids(val)]\n",
    "        else:\n",
    "            # filter to just the objs that match val\n",
    "            matched_objs = []\n",
    "            for obj_id in bucket.get_all_ids():\n",
    "                obj = self.objs.get(obj_id)\n",
    "                obj_val = getattr(obj, self.field, None)\n",
    "                if obj_val is val or obj_val == val:\n",
    "                    matched_objs.append(obj)\n",
    "            return matched_objs\n",
    "    \n",
    "    def get_obj_ids(self, val):\n",
    "        val_hash = hash(val)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        bucket = self.buckets[k]\n",
    "        \n",
    "        if isinstance(bucket, DictBucket):\n",
    "            return bucket.get_matching_ids(val)\n",
    "        else:\n",
    "            # filter to just the obj_ids that match val\n",
    "            matched_ids = []\n",
    "            for obj_id in bucket.get_all_ids():\n",
    "                obj = self.objs.get(obj_id)\n",
    "                obj_val = getattr(obj, self.field, None)\n",
    "                if obj_val is val or obj_val == val:\n",
    "                    matched_ids.append(obj)\n",
    "            return matched_ids\n",
    "    \n",
    "    def get_all_objs(self, obj_lookup):\n",
    "        return list(obj_lookup.objs.values())\n",
    "        \n",
    "    def _get_bucket_key_for(self, val_hash):\n",
    "        list_idx = self.buckets.bisect_right(val_hash) - 1\n",
    "        k, _ = self.buckets.peekitem(list_idx)\n",
    "        return k\n",
    "        \n",
    "    def _handle_big_hash_bucket(self, k):\n",
    "        # A HashBucket is over threshold. \n",
    "        # If it contains values that all hash to the same thing, make it a DictBucket.\n",
    "        # If it has many val_hashes, split it into two HashBuckets.\n",
    "        hb = self.buckets[k]\n",
    "        if len(hb.val_hash_counts) == 1:\n",
    "            # convert it to a dictbucket\n",
    "            db = DictBucket(list(hb.val_hash_counts.keys())[0], hb.obj_ids, self.objs, self.field)\n",
    "            del self.buckets[k]\n",
    "            self.buckets[db.val_hash] = db\n",
    "        else:\n",
    "            # split it into two hashbuckets\n",
    "            new_hash_counts, new_obj_ids = self.buckets[k].split(self.field, self.objs)\n",
    "            new_bucket = HashBucket()\n",
    "            new_bucket.update(new_hash_counts, new_obj_ids)\n",
    "            self.buckets[min(new_hash_counts.keys())] = new_bucket\n",
    "            \n",
    "    \n",
    "    def add(self, obj):\n",
    "        val = getattr(obj, self.field, None)\n",
    "        val_hash = hash(val)\n",
    "        obj_id = id(obj)\n",
    "        self.objs.add(obj_id, obj)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        if isinstance(self.buckets[k], DictBucket):\n",
    "            if val_hash == self.buckets[k].val_hash:\n",
    "                # add to dictbucket\n",
    "                self.buckets[k].add(val, obj_id)\n",
    "            else:\n",
    "                # can't put it in this dictbucket, the val_hash doesn't match.\n",
    "                # Make a new hashbucket to hold this item. \n",
    "                self.buckets[k+1] = HashBucket()\n",
    "                self.buckets[k+1].add(val_hash, obj_id)\n",
    "        else:\n",
    "            # add to hashbucket\n",
    "            self.buckets[k].add(val_hash, obj_id)\n",
    "                \n",
    "        if isinstance(self.buckets[k], HashBucket) and len(self.buckets[k]) >= SIZE_THRESH:\n",
    "            self._handle_big_hash_bucket(k)\n",
    "        \n",
    "    def remove(self, val, obj_id):\n",
    "        val_hash = hash(val)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        if isinstance(self.buckets[k], HashBucket):\n",
    "            self.buckets[k].remove(val_hash, obj_id)\n",
    "        else:\n",
    "            self.buckets[k].remove(val, obj_id)\n",
    "        if len(self.buckets[k]) == 0 and k != HASH_MIN:\n",
    "            del self.buckets[k]\n",
    "                \n",
    "    def bucket_report(self):\n",
    "        ls = []\n",
    "        for bkey in self.buckets:\n",
    "            bucket = self.buckets[bkey]\n",
    "            bset = set()\n",
    "            for obj_id in bucket.get_all_ids():\n",
    "                o = self.objs.get(obj_id)\n",
    "                bset.add(getattr(o, self.field))\n",
    "            ls.append((bkey, bset, len(bucket), type(self.buckets[bkey]).__name__))\n",
    "        return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3411f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "766e0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 10**6\n",
    "items = [Item() for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fd05d7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding 1000000 items\n",
      "\n",
      " 2.665 seconds to build this field thing\n",
      "\n",
      "(-9138651267913408432, {'mercury'}, 989456, 'DictBucket')\n",
      "(-9138651267913408431, {'jupiter', 'neptune'}, 165, 'HashBucket')\n",
      "(-4331169577938643553, {'uranus', 'earth', 'saturn'}, 294, 'HashBucket')\n",
      "(8896414948515954251, {'venus'}, 9988, 'DictBucket')\n",
      "(9066641044505047935, {'mars'}, 97, 'HashBucket')\n"
     ]
    }
   ],
   "source": [
    "idx = MutableFieldIndex('s')\n",
    "print('adding', n, 'items')\n",
    "t0 = time.time()\n",
    "for item in items:\n",
    "    idx.add(item)\n",
    "t1 = time.time()\n",
    "print('\\n', round(t1-t0,3), 'seconds to build this field thing\\n')\n",
    "for b in idx.bucket_report():\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "71625a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169 µs ± 17.6 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "z = idx.get_obj_ids('saturn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cf25da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'uranus'\n",
    "for o in idx.get_objs(planet):\n",
    "    idx.remove(planet, id(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "019f83f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-9138651267913408432, {'mercury'}, 989564, 'DictBucket')\n",
      "(-9138651267913408431, {'jupiter', 'neptune'}, 200, 'HashBucket')\n",
      "(-4331169577938643553, {'saturn', 'earth'}, 190, 'HashBucket')\n",
      "(8896414948515954251, {'venus'}, 9833, 'DictBucket')\n",
      "(9066641044505047935, {'mars'}, 106, 'HashBucket')\n"
     ]
    }
   ],
   "source": [
    "for b in idx.bucket_report():\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "78c390a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'venus'\n",
    "for o in idx.get_objs(planet):\n",
    "    idx.remove(planet, id(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c5e840c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-9138651267913408432, {'mercury'}, 989564, 'DictBucket')\n",
      "(-9138651267913408431, {'jupiter', 'neptune'}, 200, 'HashBucket')\n",
      "(-4331169577938643553, {'saturn', 'earth'}, 190, 'HashBucket')\n",
      "(9066641044505047935, {'mars'}, 106, 'HashBucket')\n"
     ]
    }
   ],
   "source": [
    "for b in idx.bucket_report():\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9c9b04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'mars'\n",
    "for o in idx.get_objs(planet):\n",
    "    idx.remove(planet, id(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "075507c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-9138651267913408432, {'mercury'}, 989564, 'DictBucket')\n",
      "(-9138651267913408431, {'jupiter', 'neptune'}, 200, 'HashBucket')\n",
      "(-4331169577938643553, {'saturn', 'earth'}, 190, 'HashBucket')\n"
     ]
    }
   ],
   "source": [
    "for b in idx.bucket_report():\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "973d989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crap:\n",
    "    def __init__(self):\n",
    "        self.s = random.random()\n",
    "crap_items = [Crap() for _ in range(10**6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "33581cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding 1000000 items\n",
      "\n",
      " 6.597 seconds to build this field thing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = MutableFieldIndex('s')\n",
    "print('adding', n, 'items')\n",
    "t0 = time.time()\n",
    "for item in crap_items:\n",
    "    idx.add(item)\n",
    "t1 = time.time()\n",
    "print('\\n', round(t1-t0,3), 'seconds to build this field thing\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5dd18289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4757"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx.buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4383f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196 µs ± 32.1 µs per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "idx.get_objs(crap_items[0].s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f1c55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ff9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e355e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e273f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashindex import HashIndex\n",
    "\n",
    "t0 = time.time()\n",
    "hi = HashIndex(items, on='s')\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a HashIndex')\n",
    "\n",
    "hi.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "d = dict()\n",
    "for i in items:\n",
    "    if i.s not in d:\n",
    "        d[i.s] = list()\n",
    "    d[i.s].append(i)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d13a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'mercury'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2cc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = hi.find(match={'s': planet})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = idx.get(planet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = d.get(planet)\n",
    "# yikes - how is this 1000x faster? something has gone really wrong here! let's see if it's the deref lookup that's\n",
    "# costing so much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DerefDict():\n",
    "    \n",
    "    def __init__(self, items):\n",
    "        self.objs = {id(item): item for item in items}\n",
    "        self.d = dict()\n",
    "        for i in items:\n",
    "            if i.s not in self.d:\n",
    "                self.d[i.s] = list()\n",
    "            self.d[i.s].append(id(i))\n",
    "    \n",
    "    def get(self, val):\n",
    "        ids = self.d.get(val)\n",
    "        return [self.objs.get(i) for i in ids]\n",
    "\n",
    "t0 = time.time()\n",
    "dd  = DerefDict(items)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a deref dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b576f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de59d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = dd.get(planet)\n",
    "# the difference is that you are doing len(planets) dict lookups instead of just one dict lookup.\n",
    "# Can we keep the list literal around during processing instead?\n",
    "# e.g. - most of the time we will want an entire list (simple lookup, no intersection). Detect that \n",
    "# case and we've got something as good as dict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DerefDictListy():\n",
    "    def __init__(self, items):\n",
    "        self.objs = []\n",
    "        self.d = dict()\n",
    "        for i in items:\n",
    "            if i.s not in self.d:\n",
    "                self.d[i.s] = list()\n",
    "            self.d[i.s].append(len(self.objs))\n",
    "            self.objs.append(obj)\n",
    "    \n",
    "    def get(self, val):\n",
    "        ids = self.d.get(val)\n",
    "        return itemgetter(*ids)(self.objs)\n",
    "\n",
    "t0 = time.time()\n",
    "ddl  = DerefDict(items)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a deref dict, listy edition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb00b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = ddl.get(planet)\n",
    "# the difference is that you are doing len(planets) dict lookups instead of just one dict lookup.\n",
    "# Can we keep the list literal around during processing instead?\n",
    "# e.g. - most of the time we will want an entire list (simple lookup, no intersection). Detect that \n",
    "# case and we've got something as good as dict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list('abcdefg')\n",
    "isect_ids = snp.intersect(np.array([2,3,6]), np.array([1,2,3,4,5]), indices=True)[1][1]\n",
    "itemgetter(*isect_ids)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = hi.find({'s': planet})\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba691ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
