{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c31d5f",
   "metadata": {},
   "source": [
    "```\n",
    "we want to change from\n",
    "\n",
    "lookup = {value: {objs}}\n",
    "\n",
    "to \n",
    "\n",
    "consistent_hash(value) = bucket_id\n",
    "lookup = {bucket_id: {objs}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7eba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from sortedcontainers import SortedDict\n",
    "from pympler.asizeof import asizeof\n",
    "import sortednp as snp\n",
    "from cykhash import Int64Set\n",
    "from operator import itemgetter\n",
    "from typing import Callable, Union, List, Any, Tuple\n",
    "from collections import Counter, namedtuple\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5120f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e70b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_='''\n",
    "OK, so the implementation of choice is a \n",
    "SortedDict of {min_value: Bucket}\n",
    "and a heap of {size: min_value} containing the splittable buckets that are big.\n",
    "\n",
    "Bucket knows the keys it contains as well as their counts. \n",
    "If asked for a split point, it will give one that best bisects its keys. That's O(log(n)) probably.\n",
    "You can custom-write a bisection for that. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3260cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANETS = ['mercury']*10000 + ['venus']*100 + ['earth', 'mars', 'jupiter', 'saturn', 'uranus', 'neptune']\n",
    "class Item:\n",
    "    def __init__(self):\n",
    "        self.s = random.choice(PLANETS)\n",
    "        self.x = random.random()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.s} {round(self.x, 2)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567de3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862a7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d44517a0",
   "metadata": {},
   "source": [
    "Best idea is:\n",
    "    \n",
    " - hash values to lie in some big range, like uint64\n",
    " - Initially, we have like 10 buckets containing even chunks of that range (pretend we have a good hash function...)\n",
    " - Maintain a data structure with easy max and min access (sorted deque? heaps? dict?) sorted by the number of elements stored in the bucket. Maybe have another one for n_keys or something too, we don't wanna keep trying to split one bucket that has a single high-card key in it. Ugh.\n",
    " - Anyway, split the biggest bucket when it comes time for adding more buckets. When do we add more buckets? Shit.\n",
    " - Wait. When a bucket is unsplittable, we could take it outta the list. It's its own thing now. That would work.\n",
    " - We might have to put it back in someday.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0a9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85149e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1a4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec046c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44d5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize with 1 bucket spanning whole range\n",
    "# split when there are >1000 items in a splittable bucket\n",
    "n_bits_signed = sys.hash_info.hash_bits - 1  # typically 64 bits\n",
    "HASH_MIN = -2**n_bits_signed\n",
    "HASH_MAX = 2**n_bits_signed-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3228af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_THRESH = 300\n",
    "\n",
    "class HashBucket:\n",
    "    \"\"\"\n",
    "    A HashBucket contains all obj_ids that have value hashes between some min and max value.\n",
    "    When the number of items in a HashBucket reaches SIZE_THRESH, the bucket will be split\n",
    "    into two buckets.\n",
    "    If a bucket ever gets empty, delete it unless it's the leftmost one -- we need at least one\n",
    "    always.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.obj_ids = set()  # uint64\n",
    "        self.val_hash_counts = dict()  # {int64: int64} - which hashes are stored in this bucket\n",
    "    \n",
    "    def add(self, val_hash, obj_id):\n",
    "        count = self.val_hash_counts.get(val_hash, 0)\n",
    "        self.val_hash_counts[val_hash] = count+1\n",
    "        self.obj_ids.add(obj_id)\n",
    "            \n",
    "    def update(self, new_val_hash_counts, new_obj_ids):\n",
    "        for v, c in new_val_hash_counts.items():\n",
    "            count = self.val_hash_counts.get(v, 0)\n",
    "            self.val_hash_counts[v] = count + c\n",
    "        self.obj_ids = self.obj_ids.union(new_obj_ids)\n",
    "\n",
    "    def get_all_ids(self):\n",
    "        return self.obj_ids\n",
    "    \n",
    "    def remove(self, val_hash, obj_id):\n",
    "        # todo handle exceptions\n",
    "        self.val_hash_counts[val_hash] -= 1\n",
    "        self.obj_ids.remove(obj_id)\n",
    "    \n",
    "    def split(self, field, obj_lookup):\n",
    "        my_hashes = list(sorted(self.val_hash_counts.keys()))\n",
    "        # dump out the upper half of our hashes\n",
    "        half_point = len(my_hashes) // 2\n",
    "        dumped_hash_counts = {v: self.val_hash_counts[v] for v in my_hashes[half_point:]}\n",
    "        \n",
    "        # dereference each object \n",
    "        # Find the objects with field_vals that hash to any of dumped_hashes\n",
    "        # we will move their ids to the new bucket\n",
    "        dumped_obj_ids = set()\n",
    "        for obj_id in list(self.obj_ids):\n",
    "            obj = obj_lookup.get(obj_id)\n",
    "            obj_val = getattr(obj, field, None)\n",
    "            if hash(obj_val) in dumped_hash_counts:\n",
    "                dumped_obj_ids.add(obj_id)\n",
    "                self.obj_ids.remove(obj_id)\n",
    "        for dh in dumped_hash_counts:\n",
    "            del self.val_hash_counts[dh]\n",
    "        return dumped_hash_counts, dumped_obj_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.obj_ids)\n",
    "    \n",
    "\n",
    "class DictBucket:\n",
    "    \"\"\"\n",
    "    A DictBucket stores object ids corresponding to only one val_hash. Note that multiple values\n",
    "    coult have the same val_hash (collision).\n",
    "    It stores all entries in a dict of {val: obj_id_set}, so it supports lookup by field value.\n",
    "    This makes finding objects by val very fast. Unlike a HashBucket, we don't have to dereference\n",
    "    all the objects and check their values during a find(). \n",
    "    DictBucket is great when many objects have the same val. \n",
    "    \"\"\"\n",
    "    def __init__(self, val_hash, obj_ids, obj_lookup, field):\n",
    "        self.val_hash = val_hash\n",
    "        self.d = dict()\n",
    "        for obj_id in obj_ids:\n",
    "            obj = obj_lookup.get(obj_id)\n",
    "            val = getattr(obj, field, None)\n",
    "            if val in self.d:\n",
    "                self.d[val].add(obj_id)\n",
    "            else:\n",
    "                self.d[val] = set([obj_id])\n",
    "    \n",
    "    def add(self, val, obj_id):\n",
    "        obj_ids = self.d.get(val, Int64Set())\n",
    "        obj_ids.add(obj_id)\n",
    "        \n",
    "    def remove(self, val, obj_id):\n",
    "        if val not in self.d:\n",
    "            raise KeyError('Object value not in here')\n",
    "        if obj_id not in self.d[val]:\n",
    "            raise KeyError('Object ID not in here')\n",
    "        self.d[val].remove(obj_id)\n",
    "        if len(self.d[val]) == 0:\n",
    "            del self.d[val]\n",
    "\n",
    "    def get_matching_ids(self, val):\n",
    "        return self.d[val]\n",
    "    \n",
    "    def get_all_ids(self):\n",
    "        return set.union(*self.d.values())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(len(s) for s in self.d.values())\n",
    "    \n",
    "\n",
    "class ObjLookup:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.objs = dict()\n",
    "        \n",
    "    def get(self, obj_id):\n",
    "        return self.objs.get(obj_id)\n",
    "    \n",
    "    def add(self, obj_id, obj):\n",
    "        self.objs[obj_id] = obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ec3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutableFieldIndex:\n",
    "    # Stores the possible values of this field in a set of buckets\n",
    "    # Several values may be allocated to the same bucket for space efficiency reasons\n",
    "\n",
    "    def __init__(self, field: Union[Callable, str]):\n",
    "        self.buckets = SortedDict()  # O(1) add / remove, O(log(n)) find bucket for key\n",
    "        self.buckets[HASH_MIN] = HashBucket()  # always contains at least one bucket\n",
    "        self.objs = ObjLookup()  # todo move this to a higher level (?)\n",
    "        self.field = field\n",
    "    \n",
    "    def get_objs(self, val):\n",
    "        val_hash = hash(val)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        bucket = self.buckets[k]\n",
    "        \n",
    "        if isinstance(bucket, DictBucket):\n",
    "            return [self.objs.get(obj_id) for obj_id in bucket.get_matching_ids(val)]\n",
    "        else:\n",
    "            # filter to just the objs that match val\n",
    "            matched_objs = []\n",
    "            for obj_id in bucket.get_all_ids():\n",
    "                obj = self.objs.get(obj_id)\n",
    "                obj_val = getattr(obj, self.field, None)\n",
    "                if obj_val is val or obj_val == val:\n",
    "                    matched_objs.append(obj)\n",
    "            return matched_objs\n",
    "    \n",
    "    def get_obj_ids(self, val):\n",
    "        val_hash = hash(val)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        bucket = self.buckets[k]\n",
    "        \n",
    "        if isinstance(bucket, DictBucket):\n",
    "            return bucket.get_matching_ids(val)\n",
    "        else:\n",
    "            # filter to just the obj_ids that match val\n",
    "            matched_ids = []\n",
    "            for obj_id in bucket.get_all_ids():\n",
    "                obj = self.objs.get(obj_id)\n",
    "                obj_val = getattr(obj, self.field, None)\n",
    "                if obj_val is val or obj_val == val:\n",
    "                    matched_ids.append(obj)\n",
    "            return matched_ids\n",
    "    \n",
    "    def get_all_objs(self, obj_lookup):\n",
    "        return list(obj_lookup.objs.values())\n",
    "        \n",
    "    def _get_bucket_key_for(self, val_hash):\n",
    "        list_idx = self.buckets.bisect_right(val_hash) - 1\n",
    "        k, _ = self.buckets.peekitem(list_idx)\n",
    "        return k\n",
    "        \n",
    "    def _handle_big_hash_bucket(self, k):\n",
    "        # A HashBucket is over threshold. \n",
    "        # If it contains values that all hash to the same thing, make it a DictBucket.\n",
    "        # If it has many val_hashes, split it into two HashBuckets.\n",
    "        hb = self.buckets[k]\n",
    "        if len(hb.val_hash_counts) == 1:\n",
    "            # convert it to a dictbucket\n",
    "            db = DictBucket(list(hb.val_hash_counts.keys())[0], hb.obj_ids, self.objs, self.field)\n",
    "            del self.buckets[k]\n",
    "            self.buckets[db.val_hash] = db\n",
    "        else:\n",
    "            # split it into two hashbuckets\n",
    "            new_hash_counts, new_obj_ids = self.buckets[k].split(self.field, self.objs)\n",
    "            new_bucket = HashBucket()\n",
    "            new_bucket.update(new_hash_counts, new_obj_ids)\n",
    "            self.buckets[min(new_hash_counts.keys())] = new_bucket\n",
    "            \n",
    "    \n",
    "    def add(self, obj):\n",
    "        val = getattr(obj, self.field, None)\n",
    "        val_hash = hash(val)\n",
    "        obj_id = id(obj)\n",
    "        self.objs.add(obj_id, obj)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        if isinstance(self.buckets[k], DictBucket):\n",
    "            if val_hash == self.buckets[k].val_hash:\n",
    "                # add to dictbucket\n",
    "                self.buckets[k].add(val, obj_id)\n",
    "            else:\n",
    "                # can't put it in this dictbucket, the val_hash doesn't match.\n",
    "                # Make a new hashbucket to hold this item. \n",
    "                self.buckets[k+1] = HashBucket()\n",
    "                self.buckets[k+1].add(val_hash, obj_id)\n",
    "        else:\n",
    "            # add to hashbucket\n",
    "            self.buckets[k].add(val_hash, obj_id)\n",
    "                \n",
    "        if isinstance(self.buckets[k], HashBucket) and len(self.buckets[k]) > SIZE_THRESH:\n",
    "            self._handle_big_hash_bucket(k)\n",
    "        \n",
    "    def remove(self, val, obj_id):\n",
    "        val_hash = hash(val)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        if isinstance(self.buckets[k], HashBucket):\n",
    "            self.buckets[k].remove(val_hash, obj_id)\n",
    "        else:\n",
    "            self.buckets[k].remove(val, obj_id)\n",
    "        if len(self.buckets[k]) == 0 and k != HASH_MIN:\n",
    "            del self.buckets[k]\n",
    "                \n",
    "    def bucket_report(self):\n",
    "        ls = []\n",
    "        for bkey in self.buckets:\n",
    "            bucket = self.buckets[bkey]\n",
    "            bset = set()\n",
    "            for obj_id in bucket.get_all_ids():\n",
    "                o = self.objs.get(obj_id)\n",
    "                bset.add(getattr(o, self.field))\n",
    "            ls.append((bkey, bset, len(bucket), type(self.buckets[bkey]).__name__))\n",
    "        return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2976e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774ccc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10**6\n",
    "items = [Item() for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d03c1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding 1000000 items\n",
      "\n",
      " 2.929 seconds to build this field thing\n",
      "\n",
      "(-3512829874850480194, {'mars'}, 108, 'HashBucket')\n",
      "(-2059813509894186317, {'neptune'}, 100, 'HashBucket')\n",
      "(200579733044388176, {'saturn'}, 111, 'HashBucket')\n",
      "(4892474159497681586, {'mercury'}, 989785, 'DictBucket')\n",
      "(4892474159497681587, {'jupiter', 'uranus'}, 194, 'HashBucket')\n",
      "(9069431813562177245, {'venus'}, 8752, 'DictBucket')\n",
      "(9069431813562177246, {'earth'}, 81, 'HashBucket')\n"
     ]
    }
   ],
   "source": [
    "idx = MutableFieldIndex('s')\n",
    "print('adding', n, 'items')\n",
    "t0 = time.time()\n",
    "for item in items:\n",
    "    idx.add(item)\n",
    "t1 = time.time()\n",
    "print('\\n', round(t1-t0,3), 'seconds to build this field thing\\n')\n",
    "for b in idx.bucket_report():\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5bf954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e46487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would improve build time? < 1M items / second means no one's using this for 100M items.\n",
    "# How fast could it go if all we had to do was hash all the values up front, and add them to sorteddict?\n",
    "# bout 5x faster, looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e9e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492 ms ± 14.5 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 2 -r 5\n",
    "_ = sorted([hash(random.random()) for _ in range(10**6)])\n",
    "s = SortedDict()\n",
    "for i in range(10*3):\n",
    "    s[i] = set(range(10**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34bffebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu = list(x*0.9 for x in range(int(10**7)))\n",
    "class Inty:\n",
    "    def __init__(self):\n",
    "        self.s = random.choice(menu)\n",
    "        \n",
    "item_ints = [Inty() for _ in range(10**6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860aba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06 s ± 13.4 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "# trying to be smart about hashing each value is much slower than just hashing each value\n",
    "prev_hash = None\n",
    "prev_val = None\n",
    "for i, item in enumerate(sorted(item_ints, key=lambda x: x.s)):\n",
    "    if i > 0 and item.s == prev_val:\n",
    "        h = prev_hash\n",
    "    else:\n",
    "        h = hash(item.s)\n",
    "        prev_hash = h\n",
    "        prev_val = item.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a2a1abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 ms ± 12.7 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "for item in item_ints:\n",
    "    _ = hash(item.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf81e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo sort both by the argsort etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bb3e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_encode(arr: np.ndarray):\n",
    "    if len(arr) == 0:\n",
    "        return None, None, None\n",
    "    mismatch = arr[1:] != arr[:-1]\n",
    "    i = np.append(np.where(mismatch), len(arr)-1)\n",
    "    counts = np.diff(np.append(-1, i))\n",
    "    starts = np.cumsum(np.append(0, counts))[:-1]\n",
    "    return starts, counts, arr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9070d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field(obj, field):\n",
    "    if callable(field):\n",
    "        val = field(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        val = obj.get(field, None)\n",
    "    else:\n",
    "        val = getattr(obj, field, None)\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9dacee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_hashes(objs: List[Any], field: Union[Callable, str]) -> Tuple[np.array, List]:\n",
    "    \"\"\"\n",
    "    Hash the given attribute for all objs. Sort objs and hashes by the hashes.\n",
    "\n",
    "    Takes 1s for 1M items.\n",
    "    \"\"\"\n",
    "    def _get_field(obj, field):\n",
    "        if getattr(obj, 'get', None):\n",
    "            return obj.get(field, None)\n",
    "        if isinstance(field, str):\n",
    "            val = getattr(obj, field, None)\n",
    "        return val\n",
    "    \n",
    "    #hashes = np.empty((len(objs,)), dtype='int64')\n",
    "    vals = np.empty((len(objs,)), dtype='O')\n",
    "    for i, obj in enumerate(objs):\n",
    "        vals[i] = _get_field(obj, field)\n",
    "    hashes = np.fromiter((hash(val) for val in vals), dtype='int64')\n",
    "    pos = np.argsort(hashes)\n",
    "    sorted_hashes = hashes[pos]\n",
    "    sorted_objs = itemgetter(*pos)(objs)  # slow AF\n",
    "    sorted_vals = vals[pos]# itemgetter(*pos)(vals)  # slow AF\n",
    "    return sorted_vals, sorted_hashes, sorted_objs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec1ef7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06 s ± 22.6 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3  # it's 1s now, ughhh\n",
    "\n",
    "sorted_vals, sorted_hashes, sorted_objs = get_sorted_hashes(item_ints, 's')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "403cf15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246 ms ± 5.41 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3 \n",
    "hashes = np.fromiter((getattr(item, 's', None) for item in item_ints), dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "628ac6eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hashes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_452167/1091486056.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-n 3 -r 3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'np.argsort(hashes)\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2404\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2406\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mworst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hashes' is not defined"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3 \n",
    "np.argsort(hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3 \n",
    "np.zeros((10**6,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_length_encode(arr: np.ndarray):\n",
    "    if len(arr) == 0:\n",
    "        return None, None, None\n",
    "    mismatch = arr[1:] != arr[:-1]\n",
    "    i = np.append(np.where(mismatch), len(arr)-1)\n",
    "    counts = np.diff(np.append(-1, i))\n",
    "    starts = np.cumsum(np.append(0, counts))[:-1]\n",
    "    return starts, counts, arr[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3 \n",
    "starts, counts, val_hashes = run_length_encode(sorted_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3  # it's 500ms\n",
    "\n",
    "s_hash, s_obj = get_sorted_hashes(item_ints, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 3 -r 3  # it's 500ms\n",
    "# compute all hashes and sort by hash\n",
    "hashes = np.fromiter((hash(item.s) for item in item_ints), dtype='int64')\n",
    "pos = np.argsort(hashes)\n",
    "sorted_hashes = hashes[pos]\n",
    "sorted_items = itemgetter(*pos)(item_ints)  # todo: handle itemgetter scenarios with 0 or 1 objects\n",
    "starts, counts, val_hashes = run_length_encode(sorted_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10af5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def find_bucket_starts(counts, limit):\n",
    "    \"\"\"\n",
    "    Find counts of each hash in sorted_hashes via run-length encoding.\n",
    "\n",
    "    Takes 10ms for 1M objs.\n",
    "    \"\"\"\n",
    "    result = np.empty(len(counts), dtype=np.uint64)\n",
    "    total = 0\n",
    "    idx = 0\n",
    "    for i, count in enumerate(counts):\n",
    "        total += count\n",
    "        if total > limit:\n",
    "            total = 0\n",
    "            result[idx] = i\n",
    "            idx += 1\n",
    "    return result[:idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb8b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When a large object list is provided in the constructor -- i.e., Filtered(objs, on={...}) has lots of objs,\n",
    "adding the objs one at a time is naive and slow. Buckets will be created, overfilled, and split needlessly.\n",
    "There is a ~10X performance benefit to examining the objs and constructing all the needed buckets just once.\n",
    "\n",
    "The functions here provide that speedup. They have been squeezed pretty hard for performance. It matters here!\n",
    "Building the index is most likely going to be the bottleneck when working with large datasets, especially in the\n",
    "expected data-analysis niche for this library.\n",
    "\n",
    "Workflow:\n",
    " - Hash the given attribute for all objs\n",
    " - Sort the hashes\n",
    " - Get counts of each unique hash (via run-length encoding)\n",
    " - Use a cumulative sum-like algorithm to determine the span of each bucket\n",
    " - Return all information that init needs to create buckets\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Union, Callable, Any\n",
    "from filtered.utils import get_field\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def get_sorted_hashes(objs: List[Any], field: Union[Callable, str]) -> Tuple[np.array, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Hash the given attribute for all objs. Sort objs and hashes by the hashes.\n",
    "\n",
    "    Takes 450ms for 1M objs on a numeric field. May take longer if field is a Callable or is hard to hash.\n",
    "    Breakdown:\n",
    "     - 100ms to do all the get_field() calls. Cost is the part that inspects each obj to see if it's a dict.\n",
    "     - 220ms to get and hash the field for each obj. No getting around that.\n",
    "     - 100ms to sort the hashes\n",
    "     - 30ms of whatever\n",
    "    \"\"\"\n",
    "    hashes = np.fromiter((hash(get_field(obj, field)) for obj in objs), dtype='int64')\n",
    "    pos = np.argsort(hashes)\n",
    "    sorted_hashes = hashes[pos]\n",
    "    sorted_objs = itemgetter(*pos)(objs)  # todo handle itemgetter len 0, 1 weirdness\n",
    "    return sorted_hashes, sorted_objs\n",
    "\n",
    "\n",
    "def run_length_encode(sorted_hashes: np.ndarray):\n",
    "    \"\"\"\n",
    "    Find counts of each hash in sorted_hashes via run-length encoding.\n",
    "\n",
    "    Takes 10ms for 1M objs.\n",
    "    \"\"\"\n",
    "    mismatch = sorted_hashes[1:] != sorted_hashes[:-1]\n",
    "    i = np.append(np.where(mismatch), len(sorted_hashes) - 1)\n",
    "    counts = np.diff(np.append(-1, i))\n",
    "    starts = np.cumsum(np.append(0, counts))[:-1]\n",
    "    return starts, counts, sorted_hashes[i]\n",
    "\n",
    "\n",
    "@njit\n",
    "def find_bucket_starts(counts, limit):\n",
    "    \"\"\"\n",
    "    Find the start positions for each bucket via a cumulative sum that resets when limit is exceeded.\n",
    "\n",
    "    Takes about 1ms for a counts length of 1M. 300x slower without numba (noticeable on high-cardinality data)\n",
    "    \"\"\"\n",
    "    result = np.empty(len(counts), dtype=np.uint64)\n",
    "    total = 0\n",
    "    idx = 0\n",
    "    for i, count in enumerate(counts):\n",
    "        total += count\n",
    "        if total > limit:\n",
    "            total = count\n",
    "            result[idx] = i\n",
    "            idx += 1\n",
    "    return result[:idx]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BucketInfo:\n",
    "    distinct_hashes: np.ndarray\n",
    "    distinct_hash_counts: np.ndarray\n",
    "    obj_arr: np.ndarray\n",
    "    hash_arr: np.ndarray\n",
    "\n",
    "    def __str__(self):\n",
    "        d = dict(zip(self.distinct_hashes, self.distinct_hash_counts))\n",
    "        l1 = len(self.obj_arr)\n",
    "        l2 = len(self.hash_arr)\n",
    "        mh = min(self.hash_arr)\n",
    "        return f'{mh}: {l1}={l2}; ' + str(d)\n",
    "\n",
    "\n",
    "\n",
    "def compute_buckets(objs, field, bucket_size_limit):\n",
    "    sorted_hashes, sorted_objs = get_sorted_hashes(objs, field)\n",
    "    starts, counts, val_hashes = run_length_encode(sorted_hashes)\n",
    "    bucket_starts = find_bucket_starts(counts, bucket_size_limit)\n",
    "\n",
    "    bucket_infos = []\n",
    "    for i, s in enumerate(bucket_starts):\n",
    "        if i + 1 == len(bucket_starts):\n",
    "            distinct_hashes = val_hashes[s:]\n",
    "            distinct_hash_counts = counts[s:]\n",
    "            obj_arr = sorted_objs[starts[s]:]\n",
    "            hash_arr = sorted_hashes[starts[s]:]\n",
    "        else:\n",
    "            t = bucket_starts[i + 1]\n",
    "            distinct_hashes = val_hashes[s:t]\n",
    "            distinct_hash_counts = counts[s:t]\n",
    "            obj_arr = sorted_objs[starts[s]:starts[t]]\n",
    "            hash_arr = sorted_hashes[starts[s]:starts[t]]\n",
    "        bucket_infos.append(\n",
    "            BucketInfo(\n",
    "                distinct_hashes=distinct_hashes,\n",
    "                distinct_hash_counts=distinct_hash_counts,\n",
    "                obj_arr=obj_arr,\n",
    "                hash_arr=hash_arr,\n",
    "            )\n",
    "        )\n",
    "    return bucket_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "compute_buckets(item_ints, 's', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ecec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_infos = compute_buckets(item_ints, 's', 1000)\n",
    "print(len(bucket_infos))\n",
    "for b in bucket_infos[:10]:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33bbe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_hashes[:20])\n",
    "print(starts[:10]) \n",
    "print(counts[:10])\n",
    "print(val_hashes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af275fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 10\n",
    "find_bucket_starts(counts, SIZE_THRESH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55715ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d160eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637baf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc75357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('first ten: ', sorted_hashes[:20])\n",
    "print('starts', starts[:5])\n",
    "print('counts', counts[:5])\n",
    "print('vals', val_hashes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be4a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket_ranges(counts):\n",
    "    bin_items = []\n",
    "    bin_start_pos = []\n",
    "    start_pos = 0\n",
    "    csum = 0\n",
    "    prev_hash = None\n",
    "    for i, c in enumerate(counts):\n",
    "        if csum + c > SIZE_THRESH and csum > 0:\n",
    "            # need to dump current items\n",
    "            bin_items.append(csum)\n",
    "            bin_start_pos.append(start_pos)\n",
    "            csum = 0\n",
    "            start_pos = i\n",
    "        csum += c\n",
    "    if csum > 0:\n",
    "        bin_items.append(csum)\n",
    "        bin_start_pos.append(start_pos)\n",
    "    return bin_items, bin_start_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum_reset():\n",
    "    v = np.array([1., 1., 1., np.nan, 1., 1., 1., 1., np.nan, 1.])\n",
    "    n = np.isnan(v)\n",
    "    a = ~n  # todo change this\n",
    "    c = np.cumsum(a)\n",
    "    d = np.diff(np.concatenate(([0.], c[n])))\n",
    "    v[n] = -d\n",
    "    np.cumsum(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum_breach(arr, limit):\n",
    "    total = 0\n",
    "    for i, y in enumerate(arr):\n",
    "        total += y\n",
    "        if total > limit:\n",
    "            yield i\n",
    "            total = 0\n",
    "\n",
    "np.fromiter(cumsum_breach(np.array([1,2,3,1,6,6,3,1,1,4,6]), limit=5), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "np.fromiter(cumsum_breach(counts, SIZE_THRESH), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351cb7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cz = np.fromiter(cumsum_breach(counts, SIZE_THRESH), dtype=int)\n",
    "len(cz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2295b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dddf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def cumsum_breach_numba2(x, target, result):\n",
    "    total = 0\n",
    "    iterID = 0\n",
    "    for i,x_i in enumerate(x):\n",
    "        total += x_i\n",
    "        if total > target:\n",
    "            result[iterID] = i\n",
    "            iterID += 1\n",
    "            total = 0\n",
    "    return iterID\n",
    "\n",
    "def cumsum_breach_array_init(x, target):\n",
    "    x = np.asarray(x)\n",
    "    result = np.empty(len(x),dtype=np.uint64)\n",
    "    idx = cumsum_breach_numba2(x, target, result)\n",
    "    return result[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 1\n",
    "cumsum_breach_array_init(counts, SIZE_THRESH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cumsum_breach_array_init(counts, SIZE_THRESH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d5edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a vectorized way to do this.\n",
    "# First, find the dict buckets.\n",
    "# 1. make an array of zeros called start_pos_flags. Set first position is 1.\n",
    "# 2. Find my_counts > SIZE_THRESH. Set start_pos_flags to 1 for each position, and the position after it.\n",
    "# 3. Set counts to 0 for each of these.\n",
    "# Save array of dict bucket positions.\n",
    "# Next we find the hash buckets.\n",
    "# while True:\n",
    "# 1. compute cumulative sums of counts for each segment between two start_pos_flags, put it in csums\n",
    "# 2. Find the first spot where csums > SIZE_THRESH in each segment. \n",
    "# 3. break if all cumulative sums are <= SIZE_THRESH\n",
    "\n",
    "def bucket_ranges_vec(counts_, limit=SIZE_THRESH):\n",
    "    counts = np.copy(counts_)\n",
    "    start_pos_flags = np.zeros(len(counts), dtype=bool)\n",
    "    start_pos_flags[0] = True\n",
    "    \n",
    "    # mark and remove dictbuckets\n",
    "    dict_pos = np.where(counts > limit)\n",
    "    start_pos_flags[dict_pos] = True\n",
    "    for dp in dict_pos: \n",
    "        if dp+1 < len(counts):\n",
    "            start_pos_flags[dp+1] = True\n",
    "    counts[dict_pos] = 0\n",
    "    \n",
    "    # iterate adding buckets until there is no position in the\n",
    "    # cumulative sum greater than limit\n",
    "    while True:\n",
    "        csum_seg = np.zeros((len(counts),))\n",
    "        flag_pos = np.where(start_pos_flags)\n",
    "        for i in range(flag_pos):\n",
    "            if i < len(flag_pos)-1:\n",
    "                \n",
    "        overs = np.where(csum_seg > limit)\n",
    "        if not overs:\n",
    "            break\n",
    "        \n",
    "    return start_pos_flags\n",
    "    \n",
    "    \n",
    "bucket_ranges_vec(np.array([1,2,3,1,6,6,3,1,1,4,6]), limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9621e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os = np.array([])\n",
    "cs = np.cumsum(os)\n",
    "np.diff(np.where(cs>5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9663d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 3 -r 3  # 150ms when n_bins is big, otherwise fast\n",
    "\n",
    "bin_items, bin_start_pos = get_bucket_ranges(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01293bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bin_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0741a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit -n 3 -r 3  # 150ms when n_bins is big, otherwise fast\n",
    "\n",
    "# todo always have a hashbucket starting at MIN_HASH, even if first thing is a dictbucket\n",
    "buckets = SortedDict()\n",
    "for i in range(len(bin_start_pos)):\n",
    "    start_hash = val_hashes[bin_start_pos[i]]\n",
    "    b = HashBucket()  # todo fix dictbucket constructor -  DictBucket(val_hash, )\n",
    "    #b.val_hashes = sorted_hashes[]\n",
    "    buckets[start_hash] = b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = SortedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bfe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "for i in range(len(bin_start_pos)):\n",
    "    start_hash = val_hashes[bin_start_pos[i]]\n",
    "    b = HashBucket()\n",
    "    b.val_hashes = sorted_hashes[i]\n",
    "    buckets[start_hash] = b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ebf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1614090106476887972-(2**60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc6cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SortedDict()\n",
    "s[2**61] = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd75c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets[start_hash] = b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb388af",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_hashes[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteratively\n",
    "tots = np.zeros(len(counts), dtype=int)  # running total in bin\n",
    "bin_starts = np.zeros(len(counts), dtype=int)  # flagged 1 if a bin starts at that pos\n",
    "\n",
    "dict_bin_flag = np.zeros(len(counts), dtype=bool)\n",
    "counts[dict_bin_flag] = 0\n",
    "\n",
    "bigs = np.where(tots > SIZE_THRESH)[0]\n",
    "bin_starts[bigs] = 1\n",
    "for i in [-1] + list(range(len(bigs)-1)):\n",
    "    if i == -1:\n",
    "        lo = 0\n",
    "        hi = bigs[0]\n",
    "    else:\n",
    "        lo = bigs[i]+1\n",
    "        hi = bigs[i+1]\n",
    "    tots[lo:hi] = np.cumsum(counts[lo:hi])\n",
    "tots[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17b0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccbd3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a0737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3 \n",
    "np.cumsum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bin_items[:10])):\n",
    "    print(bin_items[i], bin_start_pos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b52139",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaac807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now make buckets for each range of max-1000 elements\n",
    "buckets = SortedDict()\n",
    "csum = 0\n",
    "bucket_min = HASH_MIN\n",
    "val_hash_counts = dict()\n",
    "obj_ids = []\n",
    "for i in range(len(vals)):\n",
    "    val_hash = val_hashes[i]\n",
    "    if counts[i] > SIZE_THRESH or csum + counts[i] > SIZE_THRESH:\n",
    "        # close current bucket, if any\n",
    "        if len(obj_ids):\n",
    "            b = HashBucket()\n",
    "            b.val_hash_counts = val_hash_counts\n",
    "            b.obj_ids = obj_ids\n",
    "            buckets[val_hash_min] = b\n",
    "            \n",
    "        # handle new element\n",
    "        if counts[i] > SIZE_THRESH:\n",
    "            # this thing goes in a new dict bucket\n",
    "            csum = 0\n",
    "            \n",
    "        else:\n",
    "            # start a new hash bucket to hold this thing\n",
    "            pass\n",
    "    else:\n",
    "        # continue adding to the \n",
    "        csum += counts[i]\n",
    "        val_hash_counts[val_hash] = counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105af52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f1d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa74280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sorted_hashes))\n",
    "print(type(np.asarray(sorted_hashes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build algo goes like...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3a818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77413dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "z = idx.get_obj_ids('saturn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7414e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'uranus'\n",
    "for o in idx.get_objs(planet):\n",
    "    idx.remove(planet, id(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6bd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in idx.bucket_report():\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'venus'\n",
    "for o in idx.get_objs(planet):\n",
    "    idx.remove(planet, id(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in idx.bucket_report():\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'mars'\n",
    "for o in idx.get_objs(planet):\n",
    "    idx.remove(planet, id(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in idx.bucket_report():\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfa859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crap:\n",
    "    def __init__(self):\n",
    "        self.s = random.random()\n",
    "crap_items = [Crap() for _ in range(10**6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = MutableFieldIndex('s')\n",
    "print('adding', n, 'items')\n",
    "t0 = time.time()\n",
    "for item in crap_items:\n",
    "    idx.add(item)\n",
    "t1 = time.time()\n",
    "print('\\n', round(t1-t0,3), 'seconds to build this field thing\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30044c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx.buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "idx.get_obj_ids(crap_items[0].s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b745255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808bacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b753519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b80cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from filtered import Filtered\n",
    "\n",
    "t0 = time.time()\n",
    "hi = Filtered(items, on='s')\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a Filtered')\n",
    "\n",
    "hi.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb560af",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "d = dict()\n",
    "for i in items:\n",
    "    if i.s not in d:\n",
    "        d[i.s] = list()\n",
    "    d[i.s].append(i)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ceb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'mercury'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ad249",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = hi.find(match={'s': planet})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38154ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = idx.get(planet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8421c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = d.get(planet)\n",
    "# yikes - how is this 1000x faster? something has gone really wrong here! let's see if it's the deref lookup that's\n",
    "# costing so much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DerefDict():\n",
    "    \n",
    "    def __init__(self, items):\n",
    "        self.objs = {id(item): item for item in items}\n",
    "        self.d = dict()\n",
    "        for i in items:\n",
    "            if i.s not in self.d:\n",
    "                self.d[i.s] = list()\n",
    "            self.d[i.s].append(id(i))\n",
    "    \n",
    "    def get(self, val):\n",
    "        ids = self.d.get(val)\n",
    "        return [self.objs.get(i) for i in ids]\n",
    "\n",
    "t0 = time.time()\n",
    "dd  = DerefDict(items)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a deref dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b812d49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995e4fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = dd.get(planet)\n",
    "# the difference is that you are doing len(planets) dict lookups instead of just one dict lookup.\n",
    "# Can we keep the list literal around during processing instead?\n",
    "# e.g. - most of the time we will want an entire list (simple lookup, no intersection). Detect that \n",
    "# case and we've got something as good as dict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576dcb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DerefDictListy():\n",
    "    def __init__(self, items):\n",
    "        self.objs = []\n",
    "        self.d = dict()\n",
    "        for i in items:\n",
    "            if i.s not in self.d:\n",
    "                self.d[i.s] = list()\n",
    "            self.d[i.s].append(len(self.objs))\n",
    "            self.objs.append(obj)\n",
    "    \n",
    "    def get(self, val):\n",
    "        ids = self.d.get(val)\n",
    "        return itemgetter(*ids)(self.objs)\n",
    "\n",
    "t0 = time.time()\n",
    "ddl  = DerefDict(items)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a deref dict, listy edition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = ddl.get(planet)\n",
    "# the difference is that you are doing len(planets) dict lookups instead of just one dict lookup.\n",
    "# Can we keep the list literal around during processing instead?\n",
    "# e.g. - most of the time we will want an entire list (simple lookup, no intersection). Detect that \n",
    "# case and we've got something as good as dict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list('abcdefg')\n",
    "isect_ids = snp.intersect(np.array([2,3,6]), np.array([1,2,3,4,5]), indices=True)[1][1]\n",
    "itemgetter(*isect_ids)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = hi.find({'s': planet})\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3b5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077341f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c39a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74507836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cykhash import Int64Set, Int64toInt64Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe31f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Int64toInt64Map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ecf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "m[1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ed78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
