{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c31d5f",
   "metadata": {},
   "source": [
    "```\n",
    "we want to change from\n",
    "\n",
    "lookup = {value: {objs}}\n",
    "\n",
    "to \n",
    "\n",
    "consistent_hash(value) = bucket_id\n",
    "lookup = {bucket_id: {objs}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7eba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from sortedcontainers import SortedDict\n",
    "from pympler.asizeof import asizeof\n",
    "import sortednp as snp\n",
    "from cykhash import Int64Set\n",
    "from operator import itemgetter\n",
    "from typing import Callable, Union, List\n",
    "from collections import Counter, namedtuple\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4b0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e70b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_='''\n",
    "OK, so the implementation of choice is a \n",
    "SortedDict of {min_value: Bucket}\n",
    "and a heap of {size: min_value} containing the splittable buckets that are big.\n",
    "\n",
    "Bucket knows the keys it contains as well as their counts. \n",
    "If asked for a split point, it will give one that best bisects its keys. That's O(log(n)) probably.\n",
    "You can custom-write a bisection for that. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3260cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANETS = ['mercury']*10000 + ['venus']*100 + ['earth', 'mars', 'jupiter', 'saturn', 'uranus', 'neptune']\n",
    "class Item:\n",
    "    def __init__(self):\n",
    "        self.s = random.choice(PLANETS)\n",
    "        self.x = random.random()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.s} {round(self.x, 2)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567de3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862a7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d44517a0",
   "metadata": {},
   "source": [
    "Best idea is:\n",
    "    \n",
    " - hash values to lie in some big range, like uint64\n",
    " - Initially, we have like 10 buckets containing even chunks of that range (pretend we have a good hash function...)\n",
    " - Maintain a data structure with easy max and min access (sorted deque? heaps? dict?) sorted by the number of elements stored in the bucket. Maybe have another one for n_keys or something too, we don't wanna keep trying to split one bucket that has a single high-card key in it. Ugh.\n",
    " - Anyway, split the biggest bucket when it comes time for adding more buckets. When do we add more buckets? Shit.\n",
    " - Wait. When a bucket is unsplittable, we could take it outta the list. It's its own thing now. That would work.\n",
    " - We might have to put it back in someday.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0a9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85149e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1a4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec046c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44d5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize with 1 bucket spanning whole range\n",
    "# split when there are >1000 items in a splittable bucket\n",
    "n_bits_signed = sys.hash_info.hash_bits - 1  # typically 64 bits\n",
    "HASH_MIN = -2**n_bits_signed\n",
    "HASH_MAX = 2**n_bits_signed-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3228af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_THRESH = 300\n",
    "\n",
    "class HashBucket:\n",
    "    \"\"\"\n",
    "    A HashBucket contains all obj_ids that have value hashes between some min and max value.\n",
    "    When the number of items in a HashBucket reaches SIZE_THRESH, the bucket will be split\n",
    "    into two buckets.\n",
    "    If a bucket ever gets empty, delete it unless it's the leftmost one -- we need at least one\n",
    "    always.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.obj_ids = set()  # uint64\n",
    "        self.val_hash_counts = dict()  # {int64: int64} - which hashes are stored in this bucket\n",
    "    \n",
    "    def add(self, val_hash, obj_id):\n",
    "        count = self.val_hash_counts.get(val_hash, 0)\n",
    "        self.val_hash_counts[val_hash] = count+1\n",
    "        self.obj_ids.add(obj_id)\n",
    "            \n",
    "    def update(self, new_val_hash_counts, new_obj_ids):\n",
    "        for v, c in new_val_hash_counts.items():\n",
    "            count = self.val_hash_counts.get(v, 0)\n",
    "            self.val_hash_counts[v] = count + c\n",
    "        self.obj_ids = self.obj_ids.union(new_obj_ids)\n",
    "\n",
    "    def get_all_ids(self):\n",
    "        return self.obj_ids\n",
    "    \n",
    "    def remove(self, val_hash, obj_id):\n",
    "        # todo handle exceptions\n",
    "        self.val_hash_counts[val_hash] -= 1\n",
    "        self.obj_ids.remove(obj_id)\n",
    "    \n",
    "    def split(self, field, obj_lookup):\n",
    "        my_hashes = list(sorted(self.val_hash_counts.keys()))\n",
    "        # dump out the upper half of our hashes\n",
    "        half_point = len(my_hashes) // 2\n",
    "        dumped_hash_counts = {v: self.val_hash_counts[v] for v in my_hashes[half_point:]}\n",
    "        \n",
    "        # dereference each object \n",
    "        # Find the objects with field_vals that hash to any of dumped_hashes\n",
    "        # we will move their ids to the new bucket\n",
    "        dumped_obj_ids = set()\n",
    "        for obj_id in list(self.obj_ids):\n",
    "            obj = obj_lookup.get(obj_id)\n",
    "            obj_val = getattr(obj, field, None)\n",
    "            if hash(obj_val) in dumped_hash_counts:\n",
    "                dumped_obj_ids.add(obj_id)\n",
    "                self.obj_ids.remove(obj_id)\n",
    "        for dh in dumped_hash_counts:\n",
    "            del self.val_hash_counts[dh]\n",
    "        return dumped_hash_counts, dumped_obj_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.obj_ids)\n",
    "    \n",
    "\n",
    "class DictBucket:\n",
    "    \"\"\"\n",
    "    A DictBucket stores object ids corresponding to only one val_hash. Note that multiple values\n",
    "    coult have the same val_hash (collision).\n",
    "    It stores all entries in a dict of {val: obj_id_set}, so it supports lookup by field value.\n",
    "    This makes finding objects by val very fast. Unlike a HashBucket, we don't have to dereference\n",
    "    all the objects and check their values during a find(). \n",
    "    DictBucket is great when many objects have the same val. \n",
    "    \"\"\"\n",
    "    def __init__(self, val_hash, obj_ids, obj_lookup, field):\n",
    "        self.val_hash = val_hash\n",
    "        self.d = dict()\n",
    "        for obj_id in obj_ids:\n",
    "            obj = obj_lookup.get(obj_id)\n",
    "            val = getattr(obj, field, None)\n",
    "            if val in self.d:\n",
    "                self.d[val].add(obj_id)\n",
    "            else:\n",
    "                self.d[val] = set([obj_id])\n",
    "    \n",
    "    def add(self, val, obj_id):\n",
    "        obj_ids = self.d.get(val, Int64Set())\n",
    "        obj_ids.add(obj_id)\n",
    "        \n",
    "    def remove(self, val, obj_id):\n",
    "        if val not in self.d:\n",
    "            raise KeyError('Object value not in here')\n",
    "        if obj_id not in self.d[val]:\n",
    "            raise KeyError('Object ID not in here')\n",
    "        self.d[val].remove(obj_id)\n",
    "        if len(self.d[val]) == 0:\n",
    "            del self.d[val]\n",
    "\n",
    "    def get_matching_ids(self, val):\n",
    "        return self.d[val]\n",
    "    \n",
    "    def get_all_ids(self):\n",
    "        return set.union(*self.d.values())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum(len(s) for s in self.d.values())\n",
    "    \n",
    "\n",
    "class ObjLookup:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.objs = dict()\n",
    "        \n",
    "    def get(self, obj_id):\n",
    "        return self.objs.get(obj_id)\n",
    "    \n",
    "    def add(self, obj_id, obj):\n",
    "        self.objs[obj_id] = obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ec3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutableFieldIndex:\n",
    "    # Stores the possible values of this field in a set of buckets\n",
    "    # Several values may be allocated to the same bucket for space efficiency reasons\n",
    "\n",
    "    def __init__(self, field: Union[Callable, str]):\n",
    "        self.buckets = SortedDict()  # O(1) add / remove, O(log(n)) find bucket for key\n",
    "        self.buckets[HASH_MIN] = HashBucket()  # always contains at least one bucket\n",
    "        self.objs = ObjLookup()  # todo move this to a higher level (?)\n",
    "        self.field = field\n",
    "    \n",
    "    def get_objs(self, val):\n",
    "        val_hash = hash(val)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        bucket = self.buckets[k]\n",
    "        \n",
    "        if isinstance(bucket, DictBucket):\n",
    "            return [self.objs.get(obj_id) for obj_id in bucket.get_matching_ids(val)]\n",
    "        else:\n",
    "            # filter to just the objs that match val\n",
    "            matched_objs = []\n",
    "            for obj_id in bucket.get_all_ids():\n",
    "                obj = self.objs.get(obj_id)\n",
    "                obj_val = getattr(obj, self.field, None)\n",
    "                if obj_val is val or obj_val == val:\n",
    "                    matched_objs.append(obj)\n",
    "            return matched_objs\n",
    "    \n",
    "    def get_obj_ids(self, val):\n",
    "        val_hash = hash(val)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        bucket = self.buckets[k]\n",
    "        \n",
    "        if isinstance(bucket, DictBucket):\n",
    "            return bucket.get_matching_ids(val)\n",
    "        else:\n",
    "            # filter to just the obj_ids that match val\n",
    "            matched_ids = []\n",
    "            for obj_id in bucket.get_all_ids():\n",
    "                obj = self.objs.get(obj_id)\n",
    "                obj_val = getattr(obj, self.field, None)\n",
    "                if obj_val is val or obj_val == val:\n",
    "                    matched_ids.append(obj)\n",
    "            return matched_ids\n",
    "    \n",
    "    def get_all_objs(self, obj_lookup):\n",
    "        return list(obj_lookup.objs.values())\n",
    "        \n",
    "    def _get_bucket_key_for(self, val_hash):\n",
    "        list_idx = self.buckets.bisect_right(val_hash) - 1\n",
    "        k, _ = self.buckets.peekitem(list_idx)\n",
    "        return k\n",
    "        \n",
    "    def _handle_big_hash_bucket(self, k):\n",
    "        # A HashBucket is over threshold. \n",
    "        # If it contains values that all hash to the same thing, make it a DictBucket.\n",
    "        # If it has many val_hashes, split it into two HashBuckets.\n",
    "        hb = self.buckets[k]\n",
    "        if len(hb.val_hash_counts) == 1:\n",
    "            # convert it to a dictbucket\n",
    "            db = DictBucket(list(hb.val_hash_counts.keys())[0], hb.obj_ids, self.objs, self.field)\n",
    "            del self.buckets[k]\n",
    "            self.buckets[db.val_hash] = db\n",
    "        else:\n",
    "            # split it into two hashbuckets\n",
    "            new_hash_counts, new_obj_ids = self.buckets[k].split(self.field, self.objs)\n",
    "            new_bucket = HashBucket()\n",
    "            new_bucket.update(new_hash_counts, new_obj_ids)\n",
    "            self.buckets[min(new_hash_counts.keys())] = new_bucket\n",
    "            \n",
    "    \n",
    "    def add(self, obj):\n",
    "        val = getattr(obj, self.field, None)\n",
    "        val_hash = hash(val)\n",
    "        obj_id = id(obj)\n",
    "        self.objs.add(obj_id, obj)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        if isinstance(self.buckets[k], DictBucket):\n",
    "            if val_hash == self.buckets[k].val_hash:\n",
    "                # add to dictbucket\n",
    "                self.buckets[k].add(val, obj_id)\n",
    "            else:\n",
    "                # can't put it in this dictbucket, the val_hash doesn't match.\n",
    "                # Make a new hashbucket to hold this item. \n",
    "                self.buckets[k+1] = HashBucket()\n",
    "                self.buckets[k+1].add(val_hash, obj_id)\n",
    "        else:\n",
    "            # add to hashbucket\n",
    "            self.buckets[k].add(val_hash, obj_id)\n",
    "                \n",
    "        if isinstance(self.buckets[k], HashBucket) and len(self.buckets[k]) > SIZE_THRESH:\n",
    "            self._handle_big_hash_bucket(k)\n",
    "        \n",
    "    def remove(self, val, obj_id):\n",
    "        val_hash = hash(val)\n",
    "        k = self._get_bucket_key_for(val_hash)\n",
    "        if isinstance(self.buckets[k], HashBucket):\n",
    "            self.buckets[k].remove(val_hash, obj_id)\n",
    "        else:\n",
    "            self.buckets[k].remove(val, obj_id)\n",
    "        if len(self.buckets[k]) == 0 and k != HASH_MIN:\n",
    "            del self.buckets[k]\n",
    "                \n",
    "    def bucket_report(self):\n",
    "        ls = []\n",
    "        for bkey in self.buckets:\n",
    "            bucket = self.buckets[bkey]\n",
    "            bset = set()\n",
    "            for obj_id in bucket.get_all_ids():\n",
    "                o = self.objs.get(obj_id)\n",
    "                bset.add(getattr(o, self.field))\n",
    "            ls.append((bkey, bset, len(bucket), type(self.buckets[bkey]).__name__))\n",
    "        return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2976e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774ccc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10**6\n",
    "items = [Item() for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d03c1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding 1000000 items\n",
      "\n",
      " 2.863 seconds to build this field thing\n",
      "\n",
      "(-3057177238626995881, {'uranus'}, 90, 'HashBucket')\n",
      "(1902181302418005661, {'mercury'}, 989616, 'DictBucket')\n",
      "(1902181302418005662, {'neptune', 'saturn', 'mars'}, 288, 'HashBucket')\n",
      "(8688647582771633844, {'venus'}, 9673, 'DictBucket')\n",
      "(8688647582771633845, {'earth', 'jupiter'}, 170, 'HashBucket')\n"
     ]
    }
   ],
   "source": [
    "idx = MutableFieldIndex('s')\n",
    "print('adding', n, 'items')\n",
    "t0 = time.time()\n",
    "for item in items:\n",
    "    idx.add(item)\n",
    "t1 = time.time()\n",
    "print('\\n', round(t1-t0,3), 'seconds to build this field thing\\n')\n",
    "for b in idx.bucket_report():\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5bf954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e46487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would improve build time? < 1M items / second means no one's using this for 100M items.\n",
    "# How fast could it go if all we had to do was hash all the values up front, and add them to sorteddict?\n",
    "# bout 5x faster, looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e9e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510 ms ± 11 ms per loop (mean ± std. dev. of 5 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 2 -r 5\n",
    "_ = sorted([hash(random.random()) for _ in range(10**6)])\n",
    "s = SortedDict()\n",
    "for i in range(10*3):\n",
    "    s[i] = set(range(10**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34bffebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu = list(x*0.9 for x in range(int(10**6)))\n",
    "class Inty:\n",
    "    def __init__(self):\n",
    "        self.s = random.choice(menu)\n",
    "        \n",
    "item_ints = [Inty() for _ in range(10**6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860aba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878 ms ± 7 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "# trying to be smart about hashing each value is much slower than just hashing each value\n",
    "prev_hash = None\n",
    "prev_val = None\n",
    "for i, item in enumerate(sorted(item_ints, key=lambda x: x.s)):\n",
    "    if i > 0 and item.s == prev_val:\n",
    "        h = prev_hash\n",
    "    else:\n",
    "        h = hash(item.s)\n",
    "        prev_hash = h\n",
    "        prev_val = item.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a2a1abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 ms ± 2.9 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "for item in item_ints:\n",
    "    _ = hash(item.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf81e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo sort both by the argsort etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bb3e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_encode(arr: np.ndarray):\n",
    "    if len(arr) == 0:\n",
    "        return None, None, None\n",
    "    mismatch = arr[1:] != arr[:-1]\n",
    "    i = np.append(np.where(mismatch), len(arr)-1)\n",
    "    counts = np.diff(np.append(-1, i))\n",
    "    starts = np.cumsum(np.append(0, counts))[:-1]\n",
    "    return starts, counts, arr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c82dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 3 -r 3  # it's 500ms\n",
    "# compute all hashes and sort by hash\n",
    "hashes = np.fromiter((hash(item.s) for item in item_ints), dtype='int64')\n",
    "pos = np.argsort(hashes)\n",
    "sorted_hashes = hashes[pos]\n",
    "sorted_items = itemgetter(*pos)(item_ints)  # todo: handle itemgetter scenarios with 0 or 1 objects\n",
    "starts, counts, val_hashes = run_length_encode(sorted_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6660cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632703"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ef342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84587008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1844674407377707863, 1844674407478945242,  922337203739473107,\n",
       "       1844674407478857798,  922337203739557383, 1614090106611372739,\n",
       "       1614090106476759893,              805977,  461168601870065832,\n",
       "        691752902872030564])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4510a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first ten:  [  9  36  36  45  54  63  72  81  90  90 108 117 153 162 162 162 171 189\n",
      " 189 189]\n",
      "starts [0 1 3 4 5]\n",
      "counts [1 2 1 1 1]\n",
      "vals [ 9 36 45 54 63]\n"
     ]
    }
   ],
   "source": [
    "print('first ten: ', sorted_hashes[:20])\n",
    "print('starts', starts[:5])\n",
    "print('counts', counts[:5])\n",
    "print('vals', val_hashes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9f6cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket_ranges(counts):\n",
    "    bin_items = []\n",
    "    bin_start_pos = []\n",
    "    start_pos = 0\n",
    "    csum = 0\n",
    "    prev_hash = None\n",
    "    for i, c in enumerate(counts):\n",
    "        if csum + c > SIZE_THRESH and csum > 0:\n",
    "            # need to dump current items\n",
    "            bin_items.append(csum)\n",
    "            bin_start_pos.append(start_pos)\n",
    "            csum = 0\n",
    "            start_pos = i\n",
    "        csum += c\n",
    "    if csum > 0:\n",
    "        bin_items.append(csum)\n",
    "        bin_start_pos.append(start_pos)\n",
    "    return bin_items, bin_start_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01683955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a vectorized way to do this.\n",
    "# First, find the dict buckets.\n",
    "# 1. make an array of zeros called start_pos_flags. Set first position is 1.\n",
    "# 2. Find my_counts > SIZE_THRESH. Set start_pos_flags to 1 for each position, and the position after it.\n",
    "# 3. Set counts to 0 for each of these.\n",
    "# Save array of dict bucket positions.\n",
    "# Next we find the hash buckets.\n",
    "# while True:\n",
    "# 1. compute cumulative sums of counts for each segment between two start_pos_flags, put it in csums\n",
    "# 2. Find the first spot where csums > SIZE_THRESH in each segment. \n",
    "# 3. break if all cumulative sums are <= SIZE_THRESH\n",
    "\n",
    "def vectorized_bucket_ranges():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb5ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9663d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 3 -r 3  # 150ms when n_bins is big, otherwise fast\n",
    "\n",
    "bin_items, bin_start_pos = get_bucket_ranges(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "593fc64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3339"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bin_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15b3b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#timeit -n 3 -r 3  # 150ms when n_bins is big, otherwise fast\n",
    "\n",
    "# todo always have a hashbucket starting at MIN_HASH, even if first thing is a dictbucket\n",
    "buckets = SortedDict()\n",
    "for i in range(len(bin_start_pos)):\n",
    "    start_hash = val_hashes[bin_start_pos[i]]\n",
    "    b = HashBucket()  # todo fix dictbucket constructor -  DictBucket(val_hash, )\n",
    "    #b.val_hashes = sorted_hashes[]\n",
    "    buckets[start_hash] = b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f2078d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = SortedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8535682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.65 ms ± 986 µs per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "for i in range(len(bin_start_pos)):\n",
    "    start_hash = val_hashes[bin_start_pos[i]]\n",
    "    b = HashBucket()\n",
    "    b.val_hashes = sorted_hashes[i]\n",
    "    buckets[start_hash] = b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e24d683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3339"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb651baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461168601870040996"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1614090106476887972-(2**60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4a94b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SortedDict()\n",
    "s[2**61] = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6258a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets[start_hash] = b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f2f36cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([                  0,                  27,                  36, ...,\n",
       "       2075258708346911576, 2075258708346911585, 2075258708346911621])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_hashes[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00ffd83e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_206897/176773012.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# iteratively\n",
    "tots = np.zeros(len(counts), dtype=int)  # running total in bin\n",
    "bin_starts = np.zeros(len(counts), dtype=int)  # flagged 1 if a bin starts at that pos\n",
    "\n",
    "dict_bin_flag = np.zeros(len(counts), dtype=bool)\n",
    "counts[dict_bin_flag] = 0\n",
    "\n",
    "bigs = np.where(tots > SIZE_THRESH)[0]\n",
    "bin_starts[bigs] = 1\n",
    "for i in [-1] + list(range(len(bigs)-1)):\n",
    "    if i == -1:\n",
    "        lo = 0\n",
    "        hi = bigs[0]\n",
    "    else:\n",
    "        lo = bigs[i]+1\n",
    "        hi = bigs[i+1]\n",
    "    tots[lo:hi] = np.cumsum(counts[lo:hi])\n",
    "tots[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a892d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff4eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe261a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 3 -r 3 \n",
    "np.cumsum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(bin_items[:10])):\n",
    "    print(bin_items[i], bin_start_pos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b52139",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaac807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now make buckets for each range of max-1000 elements\n",
    "buckets = SortedDict()\n",
    "csum = 0\n",
    "bucket_min = HASH_MIN\n",
    "val_hash_counts = dict()\n",
    "obj_ids = []\n",
    "for i in range(len(vals)):\n",
    "    val_hash = val_hashes[i]\n",
    "    if counts[i] > SIZE_THRESH or csum + counts[i] > SIZE_THRESH:\n",
    "        # close current bucket, if any\n",
    "        if len(obj_ids):\n",
    "            b = HashBucket()\n",
    "            b.val_hash_counts = val_hash_counts\n",
    "            b.obj_ids = obj_ids\n",
    "            buckets[val_hash_min] = b\n",
    "            \n",
    "        # handle new element\n",
    "        if counts[i] > SIZE_THRESH:\n",
    "            # this thing goes in a new dict bucket\n",
    "            csum = 0\n",
    "            \n",
    "        else:\n",
    "            # start a new hash bucket to hold this thing\n",
    "            pass\n",
    "    else:\n",
    "        # continue adding to the \n",
    "        csum += counts[i]\n",
    "        val_hash_counts[val_hash] = counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105af52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f1d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa74280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sorted_hashes))\n",
    "print(type(np.asarray(sorted_hashes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build algo goes like...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3a818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77413dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "z = idx.get_obj_ids('saturn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7414e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'uranus'\n",
    "for o in idx.get_objs(planet):\n",
    "    idx.remove(planet, id(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6bd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in idx.bucket_report():\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'venus'\n",
    "for o in idx.get_objs(planet):\n",
    "    idx.remove(planet, id(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in idx.bucket_report():\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'mars'\n",
    "for o in idx.get_objs(planet):\n",
    "    idx.remove(planet, id(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in idx.bucket_report():\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfa859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crap:\n",
    "    def __init__(self):\n",
    "        self.s = random.random()\n",
    "crap_items = [Crap() for _ in range(10**6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = MutableFieldIndex('s')\n",
    "print('adding', n, 'items')\n",
    "t0 = time.time()\n",
    "for item in crap_items:\n",
    "    idx.add(item)\n",
    "t1 = time.time()\n",
    "print('\\n', round(t1-t0,3), 'seconds to build this field thing\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30044c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx.buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "idx.get_obj_ids(crap_items[0].s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b745255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808bacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b753519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b80cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashindex import HashIndex\n",
    "\n",
    "t0 = time.time()\n",
    "hi = HashIndex(items, on='s')\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a HashIndex')\n",
    "\n",
    "hi.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb560af",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "d = dict()\n",
    "for i in items:\n",
    "    if i.s not in d:\n",
    "        d[i.s] = list()\n",
    "    d[i.s].append(i)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ceb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet = 'mercury'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ad249",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = hi.find(match={'s': planet})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38154ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = idx.get(planet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8421c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = d.get(planet)\n",
    "# yikes - how is this 1000x faster? something has gone really wrong here! let's see if it's the deref lookup that's\n",
    "# costing so much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DerefDict():\n",
    "    \n",
    "    def __init__(self, items):\n",
    "        self.objs = {id(item): item for item in items}\n",
    "        self.d = dict()\n",
    "        for i in items:\n",
    "            if i.s not in self.d:\n",
    "                self.d[i.s] = list()\n",
    "            self.d[i.s].append(id(i))\n",
    "    \n",
    "    def get(self, val):\n",
    "        ids = self.d.get(val)\n",
    "        return [self.objs.get(i) for i in ids]\n",
    "\n",
    "t0 = time.time()\n",
    "dd  = DerefDict(items)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a deref dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b812d49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995e4fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = dd.get(planet)\n",
    "# the difference is that you are doing len(planets) dict lookups instead of just one dict lookup.\n",
    "# Can we keep the list literal around during processing instead?\n",
    "# e.g. - most of the time we will want an entire list (simple lookup, no intersection). Detect that \n",
    "# case and we've got something as good as dict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576dcb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DerefDictListy():\n",
    "    def __init__(self, items):\n",
    "        self.objs = []\n",
    "        self.d = dict()\n",
    "        for i in items:\n",
    "            if i.s not in self.d:\n",
    "                self.d[i.s] = list()\n",
    "            self.d[i.s].append(len(self.objs))\n",
    "            self.objs.append(obj)\n",
    "    \n",
    "    def get(self, val):\n",
    "        ids = self.d.get(val)\n",
    "        return itemgetter(*ids)(self.objs)\n",
    "\n",
    "t0 = time.time()\n",
    "ddl  = DerefDict(items)\n",
    "t1 = time.time()\n",
    "print(t1-t0, 'seconds to build a deref dict, listy edition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "v = ddl.get(planet)\n",
    "# the difference is that you are doing len(planets) dict lookups instead of just one dict lookup.\n",
    "# Can we keep the list literal around during processing instead?\n",
    "# e.g. - most of the time we will want an entire list (simple lookup, no intersection). Detect that \n",
    "# case and we've got something as good as dict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list('abcdefg')\n",
    "isect_ids = snp.intersect(np.array([2,3,6]), np.array([1,2,3,4,5]), indices=True)[1][1]\n",
    "itemgetter(*isect_ids)(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = hi.find({'s': planet})\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3b5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077341f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c39a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74507836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154fcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
