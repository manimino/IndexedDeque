{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88fd1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Union, Callable, Any, Iterable\n",
    "from hashindex.utils import get_field\n",
    "import random\n",
    "from hashindex import FrozenHashIndex\n",
    "from cykhash import Int64Set\n",
    "import time\n",
    "from bisect import bisect_left\n",
    "\n",
    "\n",
    "def get_field(obj, field):\n",
    "    if callable(field):\n",
    "        val = field(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        val = obj.get(field, None)\n",
    "    else:\n",
    "        val = getattr(obj, field, None)\n",
    "    return val\n",
    "\n",
    "\n",
    "SIZE_THRESH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aceb1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sort_by_hash(\n",
    "    objs: Iterable[Any], field: Union[Callable, str]\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Sort objs and vals by vals.\n",
    "\n",
    "    Takes 450ms for 1M objs on a numeric field. May take longer if field is a Callable or is hard to hash.\n",
    "    Breakdown:\n",
    "     - 100ms to do all the get_field() calls. Cost is the part that inspects each obj to see if it's a dict.\n",
    "     - 220ms to get and hash the field for each obj. No getting around that.\n",
    "     - 100ms to sort the hashes\n",
    "     - 30ms of whatever\n",
    "    \"\"\"\n",
    "    hash_arr = np.empty(len(objs), dtype='int64')\n",
    "    val_arr = np.empty(len(objs), dtype='O')\n",
    "    obj_arr = np.array(objs, dtype='O')\n",
    "    for i, o in enumerate(objs):\n",
    "        val_arr[i] = get_field(o, field)\n",
    "        hash_arr[i] = hash(val_arr[i])\n",
    "        objs[i] = get_field(o, field)\n",
    "    sort_order = np.argsort(hash_arr)\n",
    "    val_arr = val_arr[sort_order]\n",
    "    obj_arr = obj_arr[sort_order]\n",
    "    return hash_arr, val_arr, obj_arr\n",
    "\n",
    "\n",
    "def group_by_val(hash_arr: np.ndarray, val_arr: np.ndarray, obj_arr: np.ndarray):\n",
    "    \"\"\"Modifies val_arr and obj_arr so that they group elements having the same value.\n",
    "\n",
    "    Does not modify hash_arr, as we won't need it past this point.\n",
    "\n",
    "    \"\"\"\n",
    "    def _group_by_val_same_hash(val_arr, obj_arr, p0, p1):\n",
    "        \"\"\"Does group_by for a subarray all having the same hash but containing >=2 distinct values.\n",
    "\n",
    "        Normal tools for doing group_by fail here.\n",
    "        - We can't assume values are sortable, so can't just sort the values and find change points.\n",
    "        - We are grouping values that have the same hash, so dict() will be inefficient.\n",
    "\n",
    "        So just making a list for each distinct value and appending the indices to it will work.\n",
    "        That will be O(n*k), where k = num of distinct values.\n",
    "        Luckily, we don't expect too many distinct values with the same hash.\n",
    "        Having more than two hashes colliding probably means the user is doing something funky, and bad\n",
    "        performance is ok in that case.\n",
    "        \"\"\"\n",
    "        distinct_vals = []\n",
    "        val_idx_lists = []  # list of list of indices. All elements in the inner list have the same val.\n",
    "        for i in range(p0, p1):\n",
    "            try:\n",
    "                idx = distinct_vals.index(val_arr[i])\n",
    "                val_idx_lists[idx].append(i)\n",
    "            except ValueError:\n",
    "                distinct_vals.append(val_arr[i])\n",
    "                val_idx_lists.append([i])\n",
    "\n",
    "        # concat the val_idx_lists to make one array of indices, like how argsort output looks\n",
    "        sort_idxs = []\n",
    "        for ixl in val_idx_lists:\n",
    "            sort_idxs.extend(ixl)\n",
    "\n",
    "        # now apply that to each array inplace\n",
    "        val_arr[p0:p1] = val_arr[sort_idxs]\n",
    "        obj_arr[p0:p1] = obj_arr[sort_idxs]\n",
    "\n",
    "    mismatch_hash = hash_arr[1:] != hash_arr[:-1]\n",
    "    hash_change_pts = np.append(np.where(mismatch_hash), len(hash_arr) - 1)\n",
    "    p0 = 0\n",
    "    for end_i in hash_change_pts:\n",
    "        p1 = end_i + 1\n",
    "        if p1-p0 > 1:\n",
    "            v = val_arr[p0]\n",
    "            non_v_values = np.where(val_arr[p0+1:p1] != v)\n",
    "            if len(non_v_values):  # False unless there's a hash collision\n",
    "                _group_by_val_same_hash(val_arr, obj_arr, p0, p1)\n",
    "        p0 = p1\n",
    "\n",
    "\n",
    "def run_length_encode(val_arr: np.ndarray):\n",
    "    \"\"\"\n",
    "    Find counts of each val in the val_arr (sorted) via run-length encoding.\n",
    "\n",
    "    Takes 10ms for 1M objs.\n",
    "    \"\"\"\n",
    "    mismatch_val = val_arr[1:] != val_arr[:-1]\n",
    "    change_pts = np.append(np.where(mismatch_val), len(val_arr) - 1)\n",
    "    counts = np.diff(np.append(-1, change_pts))\n",
    "    starts = np.cumsum(np.append(0, counts))[:-1]\n",
    "    return starts, counts, val_arr[change_pts]\n",
    "\n",
    "\n",
    "def compute_mutable_dict(objs, field):\n",
    "    \"\"\"Create a dict of {val: obj_ids}. Used when creating a mutable index.\"\"\"\n",
    "    sorted_hashes, sorted_vals, sorted_objs = hash_and_sort(objs, field)\n",
    "    group_by_val(sorted_hashes, sorted_vals, sorted_objs)\n",
    "    starts, counts, unique_vals = run_length_encode(sorted_vals)\n",
    "    d = dict()\n",
    "    for i, v in enumerate(unique_vals):\n",
    "        start = starts[i]\n",
    "        count = counts[i]\n",
    "        if counts[i] > SIZE_THRESH:\n",
    "            d[v] = Int64Set(id(obj) for obj in sorted_objs[start:start+count])\n",
    "        else:\n",
    "            d[v] = tuple(id(obj) for obj in sorted_objs[start:start+count])\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "876ab024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Collider:\n",
    "\n",
    "    VALS = list(range(10))\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n = random.choice(self.VALS)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.n % 2\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.n == other.n\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ffea8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = [Collider() for _ in range(20)]\n",
    "# objs = [{'n': i%2} for i in range(10)]\n",
    "field = 'n'\n",
    "\n",
    "sorted_hashes, sorted_vals, sorted_objs = sort_by_hash(objs, field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e70a7d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 7, 7, 7, 8, 9, 9, 9],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f564515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 7, 7, 7, 8, 9, 9, 9],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_by_val(sorted_hashes, sorted_vals, sorted_objs)\n",
    "sorted_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82e4f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = [random.random() for _ in range(10**6)]\n",
    "rarr = np.array(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b71be99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 ms ± 2.1 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "np.argsort(rarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ed86b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_order = np.argsort(rarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf82c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.58 ms ± 1.8 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 3 -r 3\n",
    "x = rarr[sort_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501e00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = [{'a': random.random()} for _ in range(10**6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f06c047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2211081981658936\n"
     ]
    }
   ],
   "source": [
    "from hashindex import FrozenHashIndex\n",
    "\n",
    "t0 = time.time()\n",
    "fh = FrozenHashIndex(objs, ['a'])\n",
    "print(time.time()-t0)\n",
    "# wow omg what trash 4.4 jfc\n",
    "# yeah you made separate lookups for each value didn't you\n",
    "# instead of ranging the little shits\n",
    "# well, could be worse... 2.0 at 0.5.2\n",
    "# and 2.0 at 0.5.1?\n",
    "# and 1.8 at 0.5.0\n",
    "# dude this idea is just slowwwww\n",
    "# can you make it dumber somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb6d4745",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [round(obj['a']*10**6) for obj in objs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78753ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274 ns ± 1.52 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bisect_left(a, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86498e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok that is a GREAT idea.\n",
    "# don't prebuild the tinies, just bisect them out of the big array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3aea3bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0137348175048828\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# just doing a big dict-of-set takes > 1 second\n",
    "# are we adding value on top of that that's worth slightly more time?\n",
    "# I think so, the RAM usage is pretty good. Don't despair.\n",
    "# 2 seconds is really fast here. You are doing great.\n",
    "t0 = time.time()\n",
    "d = dict()\n",
    "prev = None\n",
    "a = list(sorted(a))\n",
    "i = 0\n",
    "while i < len(a):\n",
    "    a_list = []\n",
    "    if i==0:\n",
    "        v = a[0]\n",
    "        prev_v = v\n",
    "    while v == prev_v:\n",
    "        a_list.append(v)\n",
    "        prev_v = v\n",
    "        i += 1\n",
    "        if i == len(a):\n",
    "            break\n",
    "        v = a[i]\n",
    "    prev_v = v\n",
    "    if len(a_list) < 100:\n",
    "        d[a_list[0]] = tuple(a_list)\n",
    "    else:\n",
    "        d[a_list[0]] = Int64Set(a_list)\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "print(sum([len(v) for v in d.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4108f9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.2484"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pympler.asizeof import asizeof\n",
    "asizeof(d) / 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82396c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208cbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
