<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>hashindex.init_helpers API documentation</title>
<meta name="description" content="When HashIndex is initialized with objects, it would be slow to add the objects one at a time. Instead, we can
analyze the list of objects, plan out â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hashindex.init_helpers</code></h1>
</header>
<section id="section-intro">
<p>When HashIndex is initialized with objects, it would be slow to add the objects one at a time. Instead, we can
analyze the list of objects, plan out which buckets will be created, and put the objects in those buckets.</p>
<p>There is a ~10X performance benefit to examining the objs and constructing all the needed buckets just once.</p>
<p>The functions here provide that speedup. They have been squeezed pretty hard for performance. It matters here!
Building the index is most likely going to be the bottleneck when working with large datasets, especially in the
expected applications for this library.</p>
<p>Workflow:
- Hash the given attribute for all objs
- Sort the hashes
- Get counts of each unique hash (via run-length encoding)
- Use a cumulative sum-like algorithm to determine the span of each bucket
- Return all information that init needs to create buckets containing the objects (a list of <code>BucketPlan</code>)</p>
<p>Running the workflow takes between 600ms (low-cardinality case) and 800ms (high-cardinality case) on a 1M-item dataset.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
When HashIndex is initialized with objects, it would be slow to add the objects one at a time. Instead, we can
analyze the list of objects, plan out which buckets will be created, and put the objects in those buckets.

There is a ~10X performance benefit to examining the objs and constructing all the needed buckets just once.

The functions here provide that speedup. They have been squeezed pretty hard for performance. It matters here!
Building the index is most likely going to be the bottleneck when working with large datasets, especially in the
expected applications for this library.

Workflow:
 - Hash the given attribute for all objs
 - Sort the hashes
 - Get counts of each unique hash (via run-length encoding)
 - Use a cumulative sum-like algorithm to determine the span of each bucket
 - Return all information that init needs to create buckets containing the objects (a list of `BucketPlan`)

Running the workflow takes between 600ms (low-cardinality case) and 800ms (high-cardinality case) on a 1M-item dataset.
&#34;&#34;&#34;

import numpy as np
from typing import Tuple, Union, Callable, Any, Iterable
from hashindex.utils import get_field
from hashindex.constants import SIZE_THRESH
from cykhash import Int64Set


def sort_by_hash(
    objs: np.ndarray, field: Union[Callable, str]
) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    Sort objs and vals by vals.

    Takes 450ms for 1M objs on a numeric field. May take longer if field is a Callable or is hard to hash.
    Breakdown:
     - 100ms to do all the get_field() calls. Cost is the part that inspects each obj to see if it&#39;s a dict.
     - 220ms to get and hash the field for each obj. No getting around that.
     - 100ms to sort the hashes
     - 30ms of whatever
    &#34;&#34;&#34;
    hash_arr = np.empty(len(objs), dtype=&#39;int64&#39;)
    val_arr = np.empty(len(objs), dtype=&#39;O&#39;)
    for i, obj in enumerate(objs):
        val_arr[i] = get_field(obj, field)
        hash_arr[i] = hash(val_arr[i])
    sort_order = np.argsort(hash_arr)
    val_arr = val_arr[sort_order]
    obj_arr = objs[sort_order]
    hash_arr = hash_arr[sort_order]
    return hash_arr, val_arr, obj_arr


def group_by_val(hash_arr: np.ndarray, val_arr: np.ndarray, obj_arr: np.ndarray):
    &#34;&#34;&#34;Modifies val_arr, hash_arr, and obj_arr so that they group elements having the same value.&#34;&#34;&#34;

    def _group_by_val_same_hash(val_arr, obj_arr, p0, p1):
        &#34;&#34;&#34;Does group_by for a subarray all having the same hash but containing &gt;=2 distinct values.

        Normal tools for doing group_by fail here.
        - We can&#39;t assume values are sortable, so can&#39;t just sort the values and find change points.
        - We are grouping values that have the same hash, so dict() will be inefficient.

        So just making a list for each distinct value and appending the indices to it will work.
        That will be O(n*k), where k = num of distinct values.
        Luckily, we don&#39;t expect too many distinct values with the same hash.
        Having more than two hashes colliding probably means the user is doing something funky, and bad
        performance is ok in that case.
        &#34;&#34;&#34;
        distinct_vals = []
        val_idx_lists = []  # list of list of indices. All elements in the inner list have the same val.
        for i in range(p0, p1):
            try:
                idx = distinct_vals.index(val_arr[i])
                val_idx_lists[idx].append(i)
            except ValueError:
                distinct_vals.append(val_arr[i])
                val_idx_lists.append([i])

        # concat the val_idx_lists to make one array of indices, like how argsort output looks
        sort_idxs = []
        for ixl in val_idx_lists:
            sort_idxs.extend(ixl)

        # now apply that to each array inplace
        val_arr[p0:p1] = val_arr[sort_idxs]
        obj_arr[p0:p1] = obj_arr[sort_idxs]
        hash_arr[p0:p1] = hash_arr[sort_idxs]

    mismatch_hash = hash_arr[1:] != hash_arr[:-1]
    hash_change_pts = np.append(np.where(mismatch_hash), len(hash_arr) - 1)
    p0 = 0
    for end_i in hash_change_pts:
        p1 = end_i + 1
        if p1-p0 &gt; 1:
            v = val_arr[p0]
            non_v_values = np.where(val_arr[p0+1:p1] != v)
            if len(non_v_values):  # False unless there&#39;s a hash collision
                _group_by_val_same_hash(val_arr, obj_arr, p0, p1)
        p0 = p1


def run_length_encode(arr: np.ndarray):
    &#34;&#34;&#34;
    Find counts of each element in the arr (sorted) via run-length encoding.

    Takes 10ms for 1M objs.
    &#34;&#34;&#34;
    mismatch_val = arr[1:] != arr[:-1]
    change_pts = np.append(np.where(mismatch_val), len(arr) - 1)
    counts = np.diff(np.append(-1, change_pts))
    starts = np.cumsum(np.append(0, counts))[:-1]
    return starts, counts, arr[change_pts]


def compute_mutable_dict(objs, field):
    &#34;&#34;&#34;Create a dict of {val: obj_ids}. Used when creating a mutable index.&#34;&#34;&#34;
    sorted_hashes, sorted_vals, sorted_objs = sort_by_hash(objs, field)
    group_by_val(sorted_hashes, sorted_vals, sorted_objs)
    starts, counts, unique_vals = run_length_encode(sorted_vals)
    d = dict()
    for i, v in enumerate(unique_vals):
        start = starts[i]
        count = counts[i]
        if counts[i] &gt; SIZE_THRESH:
            d[v] = Int64Set(id(obj) for obj in sorted_objs[start:start+count])
        else:
            d[v] = tuple(id(obj) for obj in sorted_objs[start:start+count])
    return d</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="hashindex.init_helpers.compute_mutable_dict"><code class="name flex">
<span>def <span class="ident">compute_mutable_dict</span></span>(<span>objs, field)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a dict of {val: obj_ids}. Used when creating a mutable index.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_mutable_dict(objs, field):
    &#34;&#34;&#34;Create a dict of {val: obj_ids}. Used when creating a mutable index.&#34;&#34;&#34;
    sorted_hashes, sorted_vals, sorted_objs = sort_by_hash(objs, field)
    group_by_val(sorted_hashes, sorted_vals, sorted_objs)
    starts, counts, unique_vals = run_length_encode(sorted_vals)
    d = dict()
    for i, v in enumerate(unique_vals):
        start = starts[i]
        count = counts[i]
        if counts[i] &gt; SIZE_THRESH:
            d[v] = Int64Set(id(obj) for obj in sorted_objs[start:start+count])
        else:
            d[v] = tuple(id(obj) for obj in sorted_objs[start:start+count])
    return d</code></pre>
</details>
</dd>
<dt id="hashindex.init_helpers.group_by_val"><code class="name flex">
<span>def <span class="ident">group_by_val</span></span>(<span>hash_arr:Â numpy.ndarray, val_arr:Â numpy.ndarray, obj_arr:Â numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Modifies val_arr, hash_arr, and obj_arr so that they group elements having the same value.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def group_by_val(hash_arr: np.ndarray, val_arr: np.ndarray, obj_arr: np.ndarray):
    &#34;&#34;&#34;Modifies val_arr, hash_arr, and obj_arr so that they group elements having the same value.&#34;&#34;&#34;

    def _group_by_val_same_hash(val_arr, obj_arr, p0, p1):
        &#34;&#34;&#34;Does group_by for a subarray all having the same hash but containing &gt;=2 distinct values.

        Normal tools for doing group_by fail here.
        - We can&#39;t assume values are sortable, so can&#39;t just sort the values and find change points.
        - We are grouping values that have the same hash, so dict() will be inefficient.

        So just making a list for each distinct value and appending the indices to it will work.
        That will be O(n*k), where k = num of distinct values.
        Luckily, we don&#39;t expect too many distinct values with the same hash.
        Having more than two hashes colliding probably means the user is doing something funky, and bad
        performance is ok in that case.
        &#34;&#34;&#34;
        distinct_vals = []
        val_idx_lists = []  # list of list of indices. All elements in the inner list have the same val.
        for i in range(p0, p1):
            try:
                idx = distinct_vals.index(val_arr[i])
                val_idx_lists[idx].append(i)
            except ValueError:
                distinct_vals.append(val_arr[i])
                val_idx_lists.append([i])

        # concat the val_idx_lists to make one array of indices, like how argsort output looks
        sort_idxs = []
        for ixl in val_idx_lists:
            sort_idxs.extend(ixl)

        # now apply that to each array inplace
        val_arr[p0:p1] = val_arr[sort_idxs]
        obj_arr[p0:p1] = obj_arr[sort_idxs]
        hash_arr[p0:p1] = hash_arr[sort_idxs]

    mismatch_hash = hash_arr[1:] != hash_arr[:-1]
    hash_change_pts = np.append(np.where(mismatch_hash), len(hash_arr) - 1)
    p0 = 0
    for end_i in hash_change_pts:
        p1 = end_i + 1
        if p1-p0 &gt; 1:
            v = val_arr[p0]
            non_v_values = np.where(val_arr[p0+1:p1] != v)
            if len(non_v_values):  # False unless there&#39;s a hash collision
                _group_by_val_same_hash(val_arr, obj_arr, p0, p1)
        p0 = p1</code></pre>
</details>
</dd>
<dt id="hashindex.init_helpers.run_length_encode"><code class="name flex">
<span>def <span class="ident">run_length_encode</span></span>(<span>arr:Â numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Find counts of each element in the arr (sorted) via run-length encoding.</p>
<p>Takes 10ms for 1M objs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_length_encode(arr: np.ndarray):
    &#34;&#34;&#34;
    Find counts of each element in the arr (sorted) via run-length encoding.

    Takes 10ms for 1M objs.
    &#34;&#34;&#34;
    mismatch_val = arr[1:] != arr[:-1]
    change_pts = np.append(np.where(mismatch_val), len(arr) - 1)
    counts = np.diff(np.append(-1, change_pts))
    starts = np.cumsum(np.append(0, counts))[:-1]
    return starts, counts, arr[change_pts]</code></pre>
</details>
</dd>
<dt id="hashindex.init_helpers.sort_by_hash"><code class="name flex">
<span>def <span class="ident">sort_by_hash</span></span>(<span>objs:Â numpy.ndarray, field:Â Union[str,Â Callable]) â€‘>Â Tuple[numpy.ndarray,Â numpy.ndarray,Â numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Sort objs and vals by vals.</p>
<p>Takes 450ms for 1M objs on a numeric field. May take longer if field is a Callable or is hard to hash.
Breakdown:
- 100ms to do all the get_field() calls. Cost is the part that inspects each obj to see if it's a dict.
- 220ms to get and hash the field for each obj. No getting around that.
- 100ms to sort the hashes
- 30ms of whatever</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sort_by_hash(
    objs: np.ndarray, field: Union[Callable, str]
) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    Sort objs and vals by vals.

    Takes 450ms for 1M objs on a numeric field. May take longer if field is a Callable or is hard to hash.
    Breakdown:
     - 100ms to do all the get_field() calls. Cost is the part that inspects each obj to see if it&#39;s a dict.
     - 220ms to get and hash the field for each obj. No getting around that.
     - 100ms to sort the hashes
     - 30ms of whatever
    &#34;&#34;&#34;
    hash_arr = np.empty(len(objs), dtype=&#39;int64&#39;)
    val_arr = np.empty(len(objs), dtype=&#39;O&#39;)
    for i, obj in enumerate(objs):
        val_arr[i] = get_field(obj, field)
        hash_arr[i] = hash(val_arr[i])
    sort_order = np.argsort(hash_arr)
    val_arr = val_arr[sort_order]
    obj_arr = objs[sort_order]
    hash_arr = hash_arr[sort_order]
    return hash_arr, val_arr, obj_arr</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hashindex" href="index.html">hashindex</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="hashindex.init_helpers.compute_mutable_dict" href="#hashindex.init_helpers.compute_mutable_dict">compute_mutable_dict</a></code></li>
<li><code><a title="hashindex.init_helpers.group_by_val" href="#hashindex.init_helpers.group_by_val">group_by_val</a></code></li>
<li><code><a title="hashindex.init_helpers.run_length_encode" href="#hashindex.init_helpers.run_length_encode">run_length_encode</a></code></li>
<li><code><a title="hashindex.init_helpers.sort_by_hash" href="#hashindex.init_helpers.sort_by_hash">sort_by_hash</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>